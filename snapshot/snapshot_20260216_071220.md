# GoML v3 Snapshot - Mon Feb 16 07:12:20 AM UTC 2026

## Project Structure
```
.
├── autograd
│   ├── backward.go
│   └── helpers.go
├── backend
│   ├── backend.go
│   └── cpu
│       ├── backend.go
│       └── storage.go
├── cmd
│   ├── gradcheck
│   │   └── main.go
│   ├── mingrad
│   │   └── main.go
│   └── simpletrain
│       └── main.go
├── core
│   ├── dtype.go
│   └── shape.go
├── go.mod
├── main.go
├── nn
│   ├── attention.go
│   ├── backward_attn.go
│   ├── backward.go
│   ├── embedding.go
│   ├── feedforward.go
│   ├── layernorm.go
│   ├── linear.go
│   ├── loss.go
│   ├── model.go
│   ├── optimizer.go
│   └── transformer.go
├── ops
│   ├── loss.go
│   └── ops.go
├── optim
│   └── adamw.go
├── ssh
│   └── snapshots.sh
├── tensor
│   ├── mem.go
│   ├── reexport.go
│   └── tensor.go
├── tokenizer
│   └── byte.go
└── train
    └── trainer.go
```

# C++ Headers

# CUDA Files

# C++ Source Files

# CMake Files

# Go Files

## File: ./autograd/backward.go
```go
package autograd

import (
	"github.com/vugar/goml/tensor"
)

// Backward computes gradients for all leaf tensors that require grad.
// loss must be a scalar tensor (1 element).
func Backward(loss *tensor.Tensor) error {
	if loss.NumElements() != 1 {
		panic("backward requires scalar loss")
	}

	// Initialize grad of loss as 1.0
	onesGrad, err := tensor.Ones(loss.Shape(), loss.DType(), loss.Device())
	if err != nil {
		return err
	}

	// Topological sort (reverse)
	visited := make(map[*tensor.Tensor]bool)
	var order []*tensor.Tensor
	var topoSort func(t *tensor.Tensor)
	topoSort = func(t *tensor.Tensor) {
		if visited[t] {
			return
		}
		visited[t] = true
		if t.GradFn() != nil {
			for _, input := range t.GradFn().Inputs() {
				topoSort(input)
			}
		}
		order = append(order, t)
	}
	topoSort(loss)

	// Assign initial gradient
	gradMap := make(map[*tensor.Tensor]*tensor.Tensor)
	gradMap[loss] = onesGrad

	// Backward pass in reverse topological order
	for i := len(order) - 1; i >= 0; i-- {
		t := order[i]
		grad, ok := gradMap[t]
		if !ok || t.GradFn() == nil {
			continue
		}

		inputGrads := t.GradFn().Backward(grad)
		inputs := t.GradFn().Inputs()

		for j, input := range inputs {
			if j >= len(inputGrads) || inputGrads[j] == nil {
				continue
			}
			if existing, ok := gradMap[input]; ok {
				// Accumulate gradients
				accumulated, err := AccumulateGrad(existing, inputGrads[j])
				if err != nil {
					return err
				}
				gradMap[input] = accumulated
			} else {
				gradMap[input] = inputGrads[j]
			}
		}
	}

	// Assign gradients to leaf tensors
	for t, grad := range gradMap {
		if t.IsLeaf() && t.RequiresGrad() {
			setGrad(t, grad)
		}
	}

	return nil
}

// setGrad assigns the gradient to a tensor using the exported method.
func setGrad(t *tensor.Tensor, grad *tensor.Tensor) {
	t.SetGrad(grad)
}

// AccumulateGrad adds two gradient tensors element-wise.
func AccumulateGrad(a, b *tensor.Tensor) (*tensor.Tensor, error) {
	// Use the ops package to add - but to avoid circular imports,
	// we do it at the storage level directly
	return AddTensors(a, b)
}
```

## File: ./autograd/helpers.go
```go
package autograd

import (
	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/tensor"
)

// AddTensors adds two tensors element-wise for gradient accumulation.
func AddTensors(a, b *tensor.Tensor) (*tensor.Tensor, error) {
	bk, err := backend.GetForDevice(a.Device())
	if err != nil {
		return nil, err
	}

	outShape, err := tensor.BroadcastShapes(a.Shape(), b.Shape())
	if err != nil {
		return nil, err
	}

	n := outShape.NumElements()
	store, err := bk.Alloc(n * int(a.DType().Size()))
	if err != nil {
		return nil, err
	}

	err = bk.Add(store, a.Storage(), b.Storage(), a.Shape(), b.Shape(), outShape, a.DType())
	if err != nil {
		store.Free()
		return nil, err
	}

	return tensor.NewTensor(store, outShape, a.DType()), nil
}
```

## File: ./backend/backend.go
```go
package backend

import (
	"fmt"

	"github.com/vugar/goml/core"
)

// DeviceType represents the compute device.
type DeviceType uint8

const (
	CPU DeviceType = iota
	CUDA
	ROCm
	Metal
	Vulkan
)

func (d DeviceType) String() string {
	names := [...]string{"cpu", "cuda", "rocm", "metal", "vulkan"}
	if int(d) < len(names) {
		return names[d]
	}
	return fmt.Sprintf("device(%d)", d)
}

// Device identifies a specific device (type + index).
type Device struct {
	Type  DeviceType
	Index int // GPU index, 0 for CPU
}

var CPU0 = Device{Type: CPU, Index: 0}

func CUDADevice(index int) Device { return Device{Type: CUDA, Index: index} }
func ROCmDevice(index int) Device { return Device{Type: ROCm, Index: index} }
func MetalDevice(index int) Device { return Device{Type: Metal, Index: index} }

func (d Device) String() string {
	if d.Type == CPU {
		return "cpu"
	}
	return fmt.Sprintf("%s:%d", d.Type, d.Index)
}

// Storage represents a raw memory buffer on a device.
type Storage interface {
	// Device returns which device this storage lives on.
	Device() Device

	// Ptr returns the raw pointer to the data.
	// For CPU this is a Go pointer, for GPU it's a device pointer.
	Ptr() uintptr

	// Bytes returns the underlying byte slice (CPU only, nil for GPU).
	Bytes() []byte

	// ByteLen returns the total size in bytes.
	ByteLen() int

	// Free releases the memory.
	Free()
}

// Backend defines the compute interface that all hardware backends must implement.
// Each operation takes raw storage pointers and shape metadata.
type Backend interface {
	// Device info
	Name() string
	DeviceType() DeviceType

	// Memory management
	Alloc(byteLen int) (Storage, error)
	Free(s Storage)
	Copy(dst, src Storage, byteLen int) error
	ToDevice(dst Device, src Storage) (Storage, error) // cross-device transfer

	// Unary ops
	Neg(dst, src Storage, shape core.Shape, dtype core.DType) error
	Abs(dst, src Storage, shape core.Shape, dtype core.DType) error
	Exp(dst, src Storage, shape core.Shape, dtype core.DType) error
	Log(dst, src Storage, shape core.Shape, dtype core.DType) error
	Sqrt(dst, src Storage, shape core.Shape, dtype core.DType) error
	Tanh(dst, src Storage, shape core.Shape, dtype core.DType) error
	Relu(dst, src Storage, shape core.Shape, dtype core.DType) error
	Gelu(dst, src Storage, shape core.Shape, dtype core.DType) error
	Sigmoid(dst, src Storage, shape core.Shape, dtype core.DType) error
	Silu(dst, src Storage, shape core.Shape, dtype core.DType) error

	// Binary ops (with broadcasting)
	Add(dst, a, b Storage, shapeA, shapeB, shapeOut core.Shape, dtype core.DType) error
	Sub(dst, a, b Storage, shapeA, shapeB, shapeOut core.Shape, dtype core.DType) error
	Mul(dst, a, b Storage, shapeA, shapeB, shapeOut core.Shape, dtype core.DType) error
	Div(dst, a, b Storage, shapeA, shapeB, shapeOut core.Shape, dtype core.DType) error

	// Reduction ops
	Sum(dst, src Storage, shape core.Shape, axes []int, keepDim bool, dtype core.DType) error
	Max(dst, src Storage, shape core.Shape, axes []int, keepDim bool, dtype core.DType) error
	Mean(dst, src Storage, shape core.Shape, axes []int, keepDim bool, dtype core.DType) error

	// MatMul: C = A @ B
	// A: [M, K], B: [K, N], C: [M, N]
	// Batched: [..., M, K] @ [..., K, N] = [..., M, N]
	MatMul(dst, a, b Storage, shapeA, shapeB core.Shape, dtype core.DType) error

	// Softmax along given axis
	Softmax(dst, src Storage, shape core.Shape, axis int, dtype core.DType) error

	// LayerNorm: y = (x - mean) / sqrt(var + eps) * gamma + beta
	LayerNorm(dst, src, gamma, beta Storage, shape core.Shape, normAxis int, eps float64, dtype core.DType) error

	// Embedding lookup
	Embedding(dst, weight, indices Storage, vocabSize, embedDim, seqLen int, dtype core.DType) error

	// RoPE (Rotary Positional Embedding)
	RoPE(dst, src Storage, shape core.Shape, headDim int, base float64, dtype core.DType) error

	// Attention: scaled dot-product attention with causal mask
	ScaledDotProductAttention(
		dst, q, k, v Storage,
		batchSize, numHeads, seqLen, headDim int,
		causal bool, dtype core.DType,
	) error

	// Fill ops
	Fill(dst Storage, shape core.Shape, value float64, dtype core.DType) error
	Arange(dst Storage, start, step float64, n int, dtype core.DType) error

	// Comparison
	Where(dst, cond, a, b Storage, shape core.Shape, dtype core.DType) error
}

// Registry holds all available backends.
var registry = map[DeviceType]Backend{}

// Register adds a backend to the global registry.
func Register(b Backend) {
	registry[b.DeviceType()] = b
}

// Get returns the backend for a device type.
func Get(dt DeviceType) (Backend, error) {
	b, ok := registry[dt]
	if !ok {
		return nil, fmt.Errorf("backend %s not registered", dt)
	}
	return b, nil
}

// GetForDevice returns the backend for a specific device.
func GetForDevice(d Device) (Backend, error) {
	return Get(d.Type)
}
```

## File: ./backend/cpu/backend.go
```go
package cpu

import (
	"fmt"
	"math"
	"unsafe"

	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/core"
)

// Backend implements backend.Backend for CPU.
type Backend struct{}

func init() {
	backend.Register(&Backend{})
}

func (b *Backend) Name() string              { return "cpu" }
func (b *Backend) DeviceType() backend.DeviceType { return backend.CPU }

// ---- Memory ----

func (b *Backend) Alloc(byteLen int) (backend.Storage, error) {
	return newStorage(byteLen), nil
}

func (b *Backend) Free(s backend.Storage) {
	s.Free()
}

func (b *Backend) Copy(dst, src backend.Storage, byteLen int) error {
	d := asBytes(dst, byteLen)
	s := asBytes(src, byteLen)
	copy(d, s)
	return nil
}

func (b *Backend) ToDevice(dst backend.Device, src backend.Storage) (backend.Storage, error) {
	if dst.Type != backend.CPU {
		return nil, fmt.Errorf("cpu backend can only transfer to cpu")
	}
	newStore := newStorage(src.ByteLen())
	copy(asBytes(newStore, src.ByteLen()), asBytes(src, src.ByteLen()))
	return newStore, nil
}

// ---- Unary ops ----

func (b *Backend) Neg(dst, src backend.Storage, shape core.Shape, dtype core.DType) error {
	return unaryOp(dst, src, shape, dtype, func(x float32) float32 { return -x })
}

func (b *Backend) Abs(dst, src backend.Storage, shape core.Shape, dtype core.DType) error {
	return unaryOp(dst, src, shape, dtype, func(x float32) float32 {
		if x < 0 {
			return -x
		}
		return x
	})
}

func (b *Backend) Exp(dst, src backend.Storage, shape core.Shape, dtype core.DType) error {
	return unaryOp(dst, src, shape, dtype, func(x float32) float32 {
		return float32(math.Exp(float64(x)))
	})
}

func (b *Backend) Log(dst, src backend.Storage, shape core.Shape, dtype core.DType) error {
	return unaryOp(dst, src, shape, dtype, func(x float32) float32 {
		return float32(math.Log(float64(x)))
	})
}

func (b *Backend) Sqrt(dst, src backend.Storage, shape core.Shape, dtype core.DType) error {
	return unaryOp(dst, src, shape, dtype, func(x float32) float32 {
		return float32(math.Sqrt(float64(x)))
	})
}

func (b *Backend) Tanh(dst, src backend.Storage, shape core.Shape, dtype core.DType) error {
	return unaryOp(dst, src, shape, dtype, func(x float32) float32 {
		return float32(math.Tanh(float64(x)))
	})
}

func (b *Backend) Relu(dst, src backend.Storage, shape core.Shape, dtype core.DType) error {
	return unaryOp(dst, src, shape, dtype, func(x float32) float32 {
		if x > 0 {
			return x
		}
		return 0
	})
}

func (b *Backend) Gelu(dst, src backend.Storage, shape core.Shape, dtype core.DType) error {
	// GELU(x) = 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))
	c := float32(math.Sqrt(2.0 / math.Pi))
	return unaryOp(dst, src, shape, dtype, func(x float32) float32 {
		return 0.5 * x * (1 + float32(math.Tanh(float64(c*(x+0.044715*x*x*x)))))
	})
}

func (b *Backend) Sigmoid(dst, src backend.Storage, shape core.Shape, dtype core.DType) error {
	return unaryOp(dst, src, shape, dtype, func(x float32) float32 {
		return 1.0 / (1.0 + float32(math.Exp(float64(-x))))
	})
}

func (b *Backend) Silu(dst, src backend.Storage, shape core.Shape, dtype core.DType) error {
	return unaryOp(dst, src, shape, dtype, func(x float32) float32 {
		return x / (1.0 + float32(math.Exp(float64(-x))))
	})
}

// ---- Binary ops ----

func (b *Backend) Add(dst, a, bStore backend.Storage, shapeA, shapeB, shapeOut core.Shape, dtype core.DType) error {
	return binaryOp(dst, a, bStore, shapeA, shapeB, shapeOut, dtype, func(x, y float32) float32 { return x + y })
}

func (b *Backend) Sub(dst, a, bStore backend.Storage, shapeA, shapeB, shapeOut core.Shape, dtype core.DType) error {
	return binaryOp(dst, a, bStore, shapeA, shapeB, shapeOut, dtype, func(x, y float32) float32 { return x - y })
}

func (b *Backend) Mul(dst, a, bStore backend.Storage, shapeA, shapeB, shapeOut core.Shape, dtype core.DType) error {
	return binaryOp(dst, a, bStore, shapeA, shapeB, shapeOut, dtype, func(x, y float32) float32 { return x * y })
}

func (b *Backend) Div(dst, a, bStore backend.Storage, shapeA, shapeB, shapeOut core.Shape, dtype core.DType) error {
	return binaryOp(dst, a, bStore, shapeA, shapeB, shapeOut, dtype, func(x, y float32) float32 { return x / y })
}

// ---- Reduction ops ----

func (b *Backend) Sum(dst, src backend.Storage, shape core.Shape, axes []int, keepDim bool, dtype core.DType) error {
	return reduceOp(dst, src, shape, axes, keepDim, dtype, 0, func(acc, x float32) float32 { return acc + x })
}

func (b *Backend) Max(dst, src backend.Storage, shape core.Shape, axes []int, keepDim bool, dtype core.DType) error {
	return reduceOp(dst, src, shape, axes, keepDim, dtype, -math.MaxFloat32, func(acc, x float32) float32 {
		if x > acc {
			return x
		}
		return acc
	})
}

func (b *Backend) Mean(dst, src backend.Storage, shape core.Shape, axes []int, keepDim bool, dtype core.DType) error {
	// Mean = Sum / count
	count := 1
	for _, a := range axes {
		count *= shape[a]
	}
	return reduceOp(dst, src, shape, axes, keepDim, dtype, 0, func(acc, x float32) float32 { return acc + x/float32(count) })
}

// ---- MatMul ----

func (b *Backend) MatMul(dst, a, bStore backend.Storage, shapeA, shapeB core.Shape, dtype core.DType) error {
	if dtype != core.Float32 {
		return fmt.Errorf("matmul: only float32 supported on cpu, got %s", dtype)
	}

	ndimA := len(shapeA)
	ndimB := len(shapeB)
	M := shapeA[ndimA-2]
	K := shapeA[ndimA-1]
	N := shapeB[ndimB-1]

	// Compute batch size
	batchSize := 1
	for i := 0; i < ndimA-2; i++ {
		batchSize *= shapeA[i]
	}

	aData := f32Slice(a, batchSize*M*K)
	bData := f32Slice(bStore, batchSize*K*N)
	cData := f32Slice(dst, batchSize*M*N)

	for batch := 0; batch < batchSize; batch++ {
		aOff := batch * M * K
		bOff := batch * K * N
		cOff := batch * M * N
		matmulF32(
			cData[cOff:cOff+M*N],
			aData[aOff:aOff+M*K],
			bData[bOff:bOff+K*N],
			M, K, N,
		)
	}
	return nil
}

// matmulF32 performs C = A @ B with tiling for cache efficiency.
func matmulF32(c, a, b []float32, M, K, N int) {
	// Clear output
	for i := range c {
		c[i] = 0
	}

	const tileSize = 32

	for i0 := 0; i0 < M; i0 += tileSize {
		iEnd := min(i0+tileSize, M)
		for k0 := 0; k0 < K; k0 += tileSize {
			kEnd := min(k0+tileSize, K)
			for j0 := 0; j0 < N; j0 += tileSize {
				jEnd := min(j0+tileSize, N)
				// Micro-kernel: tile multiplication
				for i := i0; i < iEnd; i++ {
					for k := k0; k < kEnd; k++ {
						aik := a[i*K+k]
						for j := j0; j < jEnd; j++ {
							c[i*N+j] += aik * b[k*N+j]
						}
					}
				}
			}
		}
	}
}

// ---- Softmax ----

func (b *Backend) Softmax(dst, src backend.Storage, shape core.Shape, axis int, dtype core.DType) error {
	if dtype != core.Float32 {
		return fmt.Errorf("softmax: only float32 supported")
	}

	n := shape.NumElements()
	srcData := f32Slice(src, n)
	dstData := f32Slice(dst, n)

	axisSize := shape[axis]
	outerSize := 1
	for i := 0; i < axis; i++ {
		outerSize *= shape[i]
	}
	innerSize := 1
	for i := axis + 1; i < len(shape); i++ {
		innerSize *= shape[i]
	}

	for outer := 0; outer < outerSize; outer++ {
		for inner := 0; inner < innerSize; inner++ {
			// Find max for numerical stability
			maxVal := float32(-math.MaxFloat32)
			for a := 0; a < axisSize; a++ {
				idx := outer*axisSize*innerSize + a*innerSize + inner
				if srcData[idx] > maxVal {
					maxVal = srcData[idx]
				}
			}
			// Exp and sum
			sumExp := float32(0)
			for a := 0; a < axisSize; a++ {
				idx := outer*axisSize*innerSize + a*innerSize + inner
				v := float32(math.Exp(float64(srcData[idx] - maxVal)))
				dstData[idx] = v
				sumExp += v
			}
			// Normalize
			for a := 0; a < axisSize; a++ {
				idx := outer*axisSize*innerSize + a*innerSize + inner
				dstData[idx] /= sumExp
			}
		}
	}
	return nil
}

// ---- LayerNorm ----

func (b *Backend) LayerNorm(dst, src, gamma, beta backend.Storage, shape core.Shape, normAxis int, eps float64, dtype core.DType) error {
	if dtype != core.Float32 {
		return fmt.Errorf("layernorm: only float32 supported")
	}

	n := shape.NumElements()
	srcData := f32Slice(src, n)
	dstData := f32Slice(dst, n)

	normSize := 1
	for i := normAxis; i < len(shape); i++ {
		normSize *= shape[i]
	}
	batchSize := n / normSize

	var gammaData, betaData []float32
	if gamma != nil {
		gammaData = f32Slice(gamma, normSize)
	}
	if beta != nil {
		betaData = f32Slice(beta, normSize)
	}

	for batch := 0; batch < batchSize; batch++ {
		off := batch * normSize

		// Compute mean
		mean := float32(0)
		for i := 0; i < normSize; i++ {
			mean += srcData[off+i]
		}
		mean /= float32(normSize)

		// Compute variance
		variance := float32(0)
		for i := 0; i < normSize; i++ {
			d := srcData[off+i] - mean
			variance += d * d
		}
		variance /= float32(normSize)

		invStd := float32(1.0 / math.Sqrt(float64(variance)+eps))

		for i := 0; i < normSize; i++ {
			normalized := (srcData[off+i] - mean) * invStd
			if gammaData != nil {
				normalized *= gammaData[i]
			}
			if betaData != nil {
				normalized += betaData[i]
			}
			dstData[off+i] = normalized
		}
	}
	return nil
}

// ---- Embedding ----

func (b *Backend) Embedding(dst, weight, indices backend.Storage, vocabSize, embedDim, seqLen int, dtype core.DType) error {
	if dtype != core.Float32 {
		return fmt.Errorf("embedding: only float32 supported")
	}

	wData := f32Slice(weight, vocabSize*embedDim)
	iData := i64Slice(indices, seqLen)
	oData := f32Slice(dst, seqLen*embedDim)

	for s := 0; s < seqLen; s++ {
		idx := int(iData[s])
		if idx < 0 || idx >= vocabSize {
			return fmt.Errorf("embedding index %d out of range [0, %d)", idx, vocabSize)
		}
		copy(oData[s*embedDim:(s+1)*embedDim], wData[idx*embedDim:(idx+1)*embedDim])
	}
	return nil
}

// ---- RoPE ----

func (b *Backend) RoPE(dst, src backend.Storage, shape core.Shape, headDim int, base float64, dtype core.DType) error {
	if dtype != core.Float32 {
		return fmt.Errorf("rope: only float32 supported")
	}

	n := shape.NumElements()
	srcData := f32Slice(src, n)
	dstData := f32Slice(dst, n)

	// shape: [batch, heads, seq, headDim]
	batch := shape[0]
	heads := shape[1]
	seqLen := shape[2]

	halfDim := headDim / 2

	for b := 0; b < batch; b++ {
		for h := 0; h < heads; h++ {
			for pos := 0; pos < seqLen; pos++ {
				off := ((b*heads+h)*seqLen + pos) * headDim
				for i := 0; i < halfDim; i++ {
					freq := 1.0 / math.Pow(base, float64(2*i)/float64(headDim))
					angle := float64(pos) * freq
					cos := float32(math.Cos(angle))
					sin := float32(math.Sin(angle))

					x0 := srcData[off+i]
					x1 := srcData[off+halfDim+i]
					dstData[off+i] = x0*cos - x1*sin
					dstData[off+halfDim+i] = x0*sin + x1*cos
				}
			}
		}
	}
	return nil
}

// ---- Scaled Dot-Product Attention ----

func (b *Backend) ScaledDotProductAttention(
	dst, q, k, v backend.Storage,
	batchSize, numHeads, seqLen, headDim int,
	causal bool, dtype core.DType,
) error {
	if dtype != core.Float32 {
		return fmt.Errorf("attention: only float32 supported")
	}

	total := batchSize * numHeads * seqLen * headDim
	qData := f32Slice(q, total)
	kData := f32Slice(k, total)
	vData := f32Slice(v, total)
	oData := f32Slice(dst, total)

	scale := float32(1.0 / math.Sqrt(float64(headDim)))
	scores := make([]float32, seqLen*seqLen)

	for b := 0; b < batchSize; b++ {
		for h := 0; h < numHeads; h++ {
			bhOff := (b*numHeads + h) * seqLen * headDim

			// Q @ K^T -> scores [seqLen, seqLen]
			for i := 0; i < seqLen; i++ {
				for j := 0; j < seqLen; j++ {
					dot := float32(0)
					for d := 0; d < headDim; d++ {
						dot += qData[bhOff+i*headDim+d] * kData[bhOff+j*headDim+d]
					}
					scores[i*seqLen+j] = dot * scale
					if causal && j > i {
						scores[i*seqLen+j] = -1e9
					}
				}
			}

			// Softmax over last dim
			for i := 0; i < seqLen; i++ {
				maxVal := float32(-math.MaxFloat32)
				for j := 0; j < seqLen; j++ {
					if scores[i*seqLen+j] > maxVal {
						maxVal = scores[i*seqLen+j]
					}
				}
				sumExp := float32(0)
				for j := 0; j < seqLen; j++ {
					scores[i*seqLen+j] = float32(math.Exp(float64(scores[i*seqLen+j] - maxVal)))
					sumExp += scores[i*seqLen+j]
				}
				for j := 0; j < seqLen; j++ {
					scores[i*seqLen+j] /= sumExp
				}
			}

			// Attn @ V -> output
			for i := 0; i < seqLen; i++ {
				for d := 0; d < headDim; d++ {
					sum := float32(0)
					for j := 0; j < seqLen; j++ {
						sum += scores[i*seqLen+j] * vData[bhOff+j*headDim+d]
					}
					oData[bhOff+i*headDim+d] = sum
				}
			}
		}
	}
	return nil
}

// ---- Fill ops ----

func (b *Backend) Fill(dst backend.Storage, shape core.Shape, value float64, dtype core.DType) error {
	n := shape.NumElements()
	switch dtype {
	case core.Float32:
		data := f32Slice(dst, n)
		v := float32(value)
		for i := range data {
			data[i] = v
		}
	case core.Float64:
		data := f64Slice(dst, n)
		for i := range data {
			data[i] = value
		}
	case core.Int32:
		data := i32Slice(dst, n)
		v := int32(value)
		for i := range data {
			data[i] = v
		}
	case core.Int64:
		data := i64Slice(dst, n)
		v := int64(value)
		for i := range data {
			data[i] = v
		}
	default:
		return fmt.Errorf("fill: unsupported dtype %s", dtype)
	}
	return nil
}

func (b *Backend) Arange(dst backend.Storage, start, step float64, n int, dtype core.DType) error {
	switch dtype {
	case core.Float32:
		data := f32Slice(dst, n)
		for i := range data {
			data[i] = float32(start + float64(i)*step)
		}
	case core.Int64:
		data := i64Slice(dst, n)
		for i := range data {
			data[i] = int64(start + float64(i)*step)
		}
	default:
		return fmt.Errorf("arange: unsupported dtype %s", dtype)
	}
	return nil
}

// ---- Where ----

func (b *Backend) Where(dst, cond, a, bStore backend.Storage, shape core.Shape, dtype core.DType) error {
	n := shape.NumElements()
	condData := cond.Bytes()[:n]

	switch dtype {
	case core.Float32:
		aData := f32Slice(a, n)
		bData := f32Slice(bStore, n)
		dData := f32Slice(dst, n)
		for i := 0; i < n; i++ {
			if condData[i] != 0 {
				dData[i] = aData[i]
			} else {
				dData[i] = bData[i]
			}
		}
	default:
		return fmt.Errorf("where: unsupported dtype %s", dtype)
	}
	return nil
}

// ---- Helpers ----

func asBytes(s backend.Storage, n int) []byte {
	b := s.Bytes()
	if b != nil {
		return b[:n]
	}
	return unsafe.Slice((*byte)(unsafe.Pointer(s.Ptr())), n)
}

func f32Slice(s backend.Storage, n int) []float32 {
	b := s.Bytes()
	if b != nil && len(b) > 0 {
		return unsafe.Slice((*float32)(unsafe.Pointer(&b[0])), n)
	}
	return unsafe.Slice((*float32)(unsafe.Pointer(s.Ptr())), n)
}

func f64Slice(s backend.Storage, n int) []float64 {
	b := s.Bytes()
	if b != nil && len(b) > 0 {
		return unsafe.Slice((*float64)(unsafe.Pointer(&b[0])), n)
	}
	return unsafe.Slice((*float64)(unsafe.Pointer(s.Ptr())), n)
}

func i32Slice(s backend.Storage, n int) []int32 {
	b := s.Bytes()
	if b != nil && len(b) > 0 {
		return unsafe.Slice((*int32)(unsafe.Pointer(&b[0])), n)
	}
	return unsafe.Slice((*int32)(unsafe.Pointer(s.Ptr())), n)
}

func i64Slice(s backend.Storage, n int) []int64 {
	b := s.Bytes()
	if b != nil && len(b) > 0 {
		return unsafe.Slice((*int64)(unsafe.Pointer(&b[0])), n)
	}
	return unsafe.Slice((*int64)(unsafe.Pointer(s.Ptr())), n)
}

// unaryOp applies a scalar function element-wise (float32 only for now).
func unaryOp(dst, src backend.Storage, shape core.Shape, dtype core.DType, fn func(float32) float32) error {
	if dtype != core.Float32 {
		return fmt.Errorf("unary op: only float32 supported, got %s", dtype)
	}
	n := shape.NumElements()
	srcData := f32Slice(src, n)
	dstData := f32Slice(dst, n)
	for i := 0; i < n; i++ {
		dstData[i] = fn(srcData[i])
	}
	return nil
}

// binaryOp applies a binary function element-wise with broadcasting.
func binaryOp(dst, aStore, bStore backend.Storage, shapeA, shapeB, shapeOut core.Shape, dtype core.DType, fn func(float32, float32) float32) error {
	if dtype != core.Float32 {
		return fmt.Errorf("binary op: only float32 supported, got %s", dtype)
	}

	nOut := shapeOut.NumElements()
	nA := shapeA.NumElements()
	nB := shapeB.NumElements()
	aData := f32Slice(aStore, nA)
	bData := f32Slice(bStore, nB)
	dData := f32Slice(dst, nOut)

	// Fast path: same shape, no broadcasting needed
	if shapeA.Equal(shapeB) {
		for i := 0; i < nOut; i++ {
			dData[i] = fn(aData[i], bData[i])
		}
		return nil
	}

	// General broadcasting
	ndim := len(shapeOut)
	indices := make([]int, ndim)

	for i := 0; i < nOut; i++ {
		// Compute broadcast indices for A and B
		idxA := 0
		idxB := 0
		strideA := 1
		strideB := 1
		for d := ndim - 1; d >= 0; d-- {
			dimA := 1
			dimB := 1
			offA := d - (ndim - len(shapeA))
			offB := d - (ndim - len(shapeB))
			if offA >= 0 {
				dimA = shapeA[offA]
			}
			if offB >= 0 {
				dimB = shapeB[offB]
			}

			aIdx := indices[d]
			bIdx := indices[d]
			if dimA == 1 {
				aIdx = 0
			}
			if dimB == 1 {
				bIdx = 0
			}

			if offA >= 0 {
				idxA += aIdx * strideA
				strideA *= dimA
			}
			if offB >= 0 {
				idxB += bIdx * strideB
				strideB *= dimB
			}
		}

		dData[i] = fn(aData[idxA], bData[idxB])

		// Increment indices
		for d := ndim - 1; d >= 0; d-- {
			indices[d]++
			if indices[d] < shapeOut[d] {
				break
			}
			indices[d] = 0
		}
	}
	return nil
}

// reduceOp performs a reduction along given axes.
func reduceOp(dst, src backend.Storage, shape core.Shape, axes []int, keepDim bool, dtype core.DType, init float32, fn func(float32, float32) float32) error {
	if dtype != core.Float32 {
		return fmt.Errorf("reduce op: only float32 supported, got %s", dtype)
	}

	n := shape.NumElements()
	srcData := f32Slice(src, n)

	// Compute output shape
	outShape := make(core.Shape, 0, len(shape))
	axisSet := make(map[int]bool)
	for _, a := range axes {
		axisSet[a] = true
	}
	for i, d := range shape {
		if axisSet[i] {
			if keepDim {
				outShape = append(outShape, 1)
			}
		} else {
			outShape = append(outShape, d)
		}
	}
	if len(outShape) == 0 {
		outShape = core.Shape{1}
	}

	nOut := outShape.NumElements()
	dstData := f32Slice(dst, nOut)

	// Initialize output
	for i := range dstData {
		dstData[i] = init
	}

	// Iterate over all source elements
	ndim := len(shape)
	indices := make([]int, ndim)

	for i := 0; i < n; i++ {
		// Compute output index
		outIdx := 0
		outStride := 1
		for d := len(outShape) - 1; d >= 0; d-- {
			// Map source dim to output dim
			srcDim := d
			if !keepDim {
				// Count how many reduced axes are before this output dim
				skip := 0
				for _, a := range axes {
					if a <= d+skip {
						skip++
					}
				}
				srcDim = d + skip
			}
			idx := indices[srcDim]
			if axisSet[srcDim] {
				idx = 0
			}
			outIdx += idx * outStride
			outStride *= outShape[d]
		}

		dstData[outIdx] = fn(dstData[outIdx], srcData[i])

		// Increment indices
		for d := ndim - 1; d >= 0; d-- {
			indices[d]++
			if indices[d] < shape[d] {
				break
			}
			indices[d] = 0
		}
	}

	return nil
}
```

## File: ./backend/cpu/storage.go
```go
package cpu

import (
	"unsafe"

	"github.com/vugar/goml/backend"
)

// storage is a CPU memory buffer backed by a Go byte slice.
type storage struct {
	data []byte
}

func newStorage(byteLen int) *storage {
	return &storage{data: make([]byte, byteLen)}
}

func (s *storage) Device() backend.Device { return backend.CPU0 }

func (s *storage) Ptr() uintptr {
	if len(s.data) == 0 {
		return 0
	}
	return uintptr(unsafe.Pointer(&s.data[0]))
}

func (s *storage) ByteLen() int { return len(s.data) }

func (s *storage) Bytes() []byte { return s.data }

func (s *storage) Free() {
	s.data = nil
}
```

## File: ./cmd/gradcheck/main.go
```go
package main

import (
	"fmt"
	"math"

	_ "github.com/vugar/goml/backend/cpu"
	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/nn"
	"github.com/vugar/goml/ops"
	"github.com/vugar/goml/tensor"
)

func main() {
	fmt.Println("=== Gradient Check ===\n")

	// Tiny model: 1 layer, dim=8, 2 heads
	cfg := nn.ModelConfig{
		VocabSize:    16,
		Dim:          8,
		NumLayers:    1,
		NumHeads:     2,
		FFNHiddenDim: 16,
		UseSwiGLU:    true,
		MaxSeqLen:    8,
		NormEps:      1e-5,
	}

	model, _ := nn.NewLLM(cfg, backend.CPU0)
	fmt.Printf("Model: %d params\n", model.CountParameters())

	// Fixed input
	inputs, _ := tensor.FromSlice([]int64{1, 3, 5, 7}, tensor.Shape{1, 4})
	targets, _ := tensor.FromSlice([]int64{3, 5, 7, 9}, tensor.Shape{1, 4})

	// Test 1: Can we compute loss?
	logits, cache, err := model.ForwardWithCache(inputs)
	if err != nil {
		panic(err)
	}
	loss, _ := ops.CrossEntropyLoss(logits, targets)
	lossVal := loss.ToFloat32Slice()[0]
	fmt.Printf("Initial loss: %.6f (expected ~%.4f = ln(%d))\n", lossVal, math.Log(float64(cfg.VocabSize)), cfg.VocabSize)

	// Test 2: Compute analytical gradients
	dLogits, _ := ops.CrossEntropyBackward(logits, targets)
	
	// Zero all grads
	for _, p := range model.Parameters() {
		p.SetGrad(nil)
	}
	
	err = model.Backward(cache, dLogits)
	if err != nil {
		panic(fmt.Sprintf("Backward error: %v", err))
	}

	// Test 3: Numerical gradient check on a few parameters
	fmt.Println("\n--- Numerical Gradient Check ---")
	eps := float64(1e-4)
	
	params := model.Parameters()
	paramsToCheck := []struct{
		name string
		idx  int
	}{
		{"output.weight", len(params) - 1},
		{"final_norm.gamma", len(params) - 3},
		{"layer0.ffn.w1.weight", 6},
		{"layer0.attn.wq.weight", 2},
		{"embedding", 0},
	}

	for _, pc := range paramsToCheck {
		if pc.idx >= len(params) {
			continue
		}
		p := params[pc.idx]
		grad := p.Grad()
		if grad == nil {
			fmt.Printf("%-25s: NO GRADIENT!\n", pc.name)
			continue
		}
		
		pData := p.ToFloat32Slice()
		gData := grad.ToFloat32Slice()
		
		// Check first 3 elements
		maxErr := float64(0)
		for j := 0; j < 3 && j < len(pData); j++ {
			original := pData[j]
			
			// f(x + eps)
			pData[j] = original + float32(eps)
			logitsPlus, _ := model.Forward(inputs)
			lossPlus, _ := ops.CrossEntropyLoss(logitsPlus, targets)
			
			// f(x - eps)
			pData[j] = original - float32(eps)
			logitsMinus, _ := model.Forward(inputs)
			lossMinus, _ := ops.CrossEntropyLoss(logitsMinus, targets)
			
			// Restore
			pData[j] = original
			
			numGrad := (float64(lossPlus.ToFloat32Slice()[0]) - float64(lossMinus.ToFloat32Slice()[0])) / (2 * eps)
			anaGrad := float64(gData[j])
			
			relErr := math.Abs(numGrad - anaGrad) / (math.Abs(numGrad) + math.Abs(anaGrad) + 1e-8)
			if relErr > maxErr {
				maxErr = relErr
			}
			
			if j == 0 {
				fmt.Printf("%-25s: ana=%.6f num=%.6f rel_err=%.6f", pc.name, anaGrad, numGrad, relErr)
			}
		}
		
		status := "✓"
		if maxErr > 0.01 {
			status = "✗ BAD"
		} else if maxErr > 0.001 {
			status = "~ OK"
		}
		fmt.Printf(" max_err=%.6f %s\n", maxErr, status)
	}

	// Test 4: Try a single gradient step and see if loss decreases
	fmt.Println("\n--- Single Step Test ---")
	fmt.Printf("Loss before: %.6f\n", lossVal)
	
	lr := float32(0.01)
	for _, p := range model.Parameters() {
		if p.Grad() == nil {
			continue
		}
		pData := p.ToFloat32Slice()
		gData := p.Grad().ToFloat32Slice()
		for i := range pData {
			pData[i] -= lr * gData[i]
		}
	}
	
	logits2, _ := model.Forward(inputs)
	loss2, _ := ops.CrossEntropyLoss(logits2, targets)
	lossVal2 := loss2.ToFloat32Slice()[0]
	fmt.Printf("Loss after:  %.6f\n", lossVal2)
	
	if lossVal2 < lossVal {
		fmt.Println("✓ Loss decreased! Gradients are correct direction.")
	} else {
		fmt.Println("✗ Loss INCREASED! Gradient direction is wrong!")
	}
}
```

## File: ./cmd/mingrad/main.go
```go
package main

import (
	"fmt"
	"math"

	_ "github.com/vugar/goml/backend/cpu"
	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/nn"
	"github.com/vugar/goml/ops"
	"github.com/vugar/goml/tensor"
)

func main() {
	fmt.Println("=== Minimal Gradient Test ===\n")

	vocabSize := 16
	dim := 8

	// Just one linear layer: input [seqLen, dim] → logits [seqLen, vocab]
	linear, _ := nn.NewLinear(dim, vocabSize, true, backend.CPU0)

	// Random input
	seqLen := 4
	xData := make([]float32, seqLen*dim)
	for i := range xData { xData[i] = float32(i)*0.1 - 1.0 }
	x, _ := tensor.FromSlice(xData, tensor.Shape{seqLen, dim})
	
	targets, _ := tensor.FromSlice([]int64{3, 5, 7, 9}, tensor.Shape{1, seqLen})

	// Forward
	logitsFlat, _ := linear.Forward(x)
	logits, _ := tensor.FromSlice(logitsFlat.ToFloat32Slice(), tensor.Shape{1, seqLen, vocabSize})
	
	loss, _ := ops.CrossEntropyLoss(logits, targets)
	lossVal := loss.ToFloat32Slice()[0]
	fmt.Printf("Loss: %.6f (expected ~%.4f)\n", lossVal, math.Log(float64(vocabSize)))

	// Backward
	dLogits, _ := ops.CrossEntropyBackward(logits, targets)
	dLogitsFlat, _ := tensor.FromSlice(dLogits.ToFloat32Slice(), tensor.Shape{seqLen, vocabSize})
	
	linear.Weight.SetGrad(nil)
	if linear.Bias != nil { linear.Bias.SetGrad(nil) }
	dx, _ := linear.Backward(x, dLogitsFlat)
	_ = dx

	// Numerical gradient check
	fmt.Println("\n--- Weight Gradient Check ---")
	eps := 1e-4
	wData := linear.Weight.ToFloat32Slice()
	wGrad := linear.Weight.Grad().ToFloat32Slice()

	maxErr := float64(0)
	for j := 0; j < 10 && j < len(wData); j++ {
		orig := wData[j]
		
		wData[j] = orig + float32(eps)
		logP, _ := linear.Forward(x)
		logP3, _ := tensor.FromSlice(logP.ToFloat32Slice(), tensor.Shape{1, seqLen, vocabSize})
		lP, _ := ops.CrossEntropyLoss(logP3, targets)
		
		wData[j] = orig - float32(eps)
		logM, _ := linear.Forward(x)
		logM3, _ := tensor.FromSlice(logM.ToFloat32Slice(), tensor.Shape{1, seqLen, vocabSize})
		lM, _ := ops.CrossEntropyLoss(logM3, targets)
		
		wData[j] = orig
		
		numGrad := (float64(lP.ToFloat32Slice()[0]) - float64(lM.ToFloat32Slice()[0])) / (2 * eps)
		anaGrad := float64(wGrad[j])
		relErr := math.Abs(numGrad-anaGrad) / (math.Abs(numGrad) + math.Abs(anaGrad) + 1e-8)
		if relErr > maxErr { maxErr = relErr }

		status := "✓"
		if relErr > 0.01 { status = "✗" }
		fmt.Printf("  w[%d]: ana=%.6f num=%.6f rel=%.6f %s\n", j, anaGrad, numGrad, relErr, status)
	}
	fmt.Printf("Max relative error: %.6f\n", maxErr)

	// Bias check
	if linear.Bias != nil && linear.Bias.Grad() != nil {
		fmt.Println("\n--- Bias Gradient Check ---")
		bData := linear.Bias.ToFloat32Slice()
		bGrad := linear.Bias.Grad().ToFloat32Slice()
		for j := 0; j < 5 && j < len(bData); j++ {
			orig := bData[j]
			
			bData[j] = orig + float32(eps)
			logP, _ := linear.Forward(x)
			logP3, _ := tensor.FromSlice(logP.ToFloat32Slice(), tensor.Shape{1, seqLen, vocabSize})
			lP, _ := ops.CrossEntropyLoss(logP3, targets)
			
			bData[j] = orig - float32(eps)
			logM, _ := linear.Forward(x)
			logM3, _ := tensor.FromSlice(logM.ToFloat32Slice(), tensor.Shape{1, seqLen, vocabSize})
			lM, _ := ops.CrossEntropyLoss(logM3, targets)
			
			bData[j] = orig
			
			numGrad := (float64(lP.ToFloat32Slice()[0]) - float64(lM.ToFloat32Slice()[0])) / (2 * eps)
			anaGrad := float64(bGrad[j])
			relErr := math.Abs(numGrad-anaGrad) / (math.Abs(numGrad) + math.Abs(anaGrad) + 1e-8)
			
			status := "✓"
			if relErr > 0.01 { status = "✗" }
			fmt.Printf("  b[%d]: ana=%.6f num=%.6f rel=%.6f %s\n", j, anaGrad, numGrad, relErr, status)
		}
	}
	
	// dx check
	fmt.Println("\n--- dx Gradient Check ---")
	dxData := dx.ToFloat32Slice()
	for j := 0; j < 5; j++ {
		orig := xData[j]
		
		xData[j] = orig + float32(eps)
		xP, _ := tensor.FromSlice(xData, tensor.Shape{seqLen, dim})
		logP, _ := linear.Forward(xP)
		logP3, _ := tensor.FromSlice(logP.ToFloat32Slice(), tensor.Shape{1, seqLen, vocabSize})
		lP, _ := ops.CrossEntropyLoss(logP3, targets)
		
		xData[j] = orig - float32(eps)
		xM, _ := tensor.FromSlice(xData, tensor.Shape{seqLen, dim})
		logM, _ := linear.Forward(xM)
		logM3, _ := tensor.FromSlice(logM.ToFloat32Slice(), tensor.Shape{1, seqLen, vocabSize})
		lM, _ := ops.CrossEntropyLoss(logM3, targets)
		
		xData[j] = orig
		
		numGrad := (float64(lP.ToFloat32Slice()[0]) - float64(lM.ToFloat32Slice()[0])) / (2 * eps)
		anaGrad := float64(dxData[j])
		relErr := math.Abs(numGrad-anaGrad) / (math.Abs(numGrad) + math.Abs(anaGrad) + 1e-8)
		
		status := "✓"
		if relErr > 0.01 { status = "✗" }
		fmt.Printf("  dx[%d]: ana=%.6f num=%.6f rel=%.6f %s\n", j, anaGrad, numGrad, relErr, status)
	}

	// Single gradient step test
	fmt.Println("\n--- Single Step Test ---")
	fmt.Printf("Loss before: %.6f\n", lossVal)
	lr := float32(0.1)
	for i := range wData {
		wData[i] -= lr * wGrad[i]
	}
	logits2Flat, _ := linear.Forward(x)
	logits2, _ := tensor.FromSlice(logits2Flat.ToFloat32Slice(), tensor.Shape{1, seqLen, vocabSize})
	loss2, _ := ops.CrossEntropyLoss(logits2, targets)
	lossVal2 := loss2.ToFloat32Slice()[0]
	fmt.Printf("Loss after:  %.6f\n", lossVal2)
	if lossVal2 < lossVal {
		fmt.Println("✓ Loss decreased!")
	} else {
		fmt.Println("✗ Loss INCREASED!")
	}
}
```

## File: ./cmd/simpletrain/main.go
```go
package main

import (
	"fmt"
	"math"
	"math/rand"
	"os"
	"time"

	_ "github.com/vugar/goml/backend/cpu"
	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/nn"
	"github.com/vugar/goml/ops"
	"github.com/vugar/goml/tensor"
	"github.com/vugar/goml/tokenizer"
)

// SimpleLM: Embedding → LayerNorm → Linear (no attention, no transformer)
// This validates the training pipeline independent of the attention backward.
type SimpleLM struct {
	Embed  *nn.Embedding
	Norm   *nn.LayerNorm
	Hidden *nn.Linear
	Act    string // "gelu"
	Output *nn.Linear
	Config nn.ModelConfig
}

func NewSimpleLM(cfg nn.ModelConfig, dev backend.Device) (*SimpleLM, error) {
	emb, _ := nn.NewEmbedding(cfg.VocabSize, cfg.Dim, dev)
	norm, _ := nn.NewLayerNorm(cfg.Dim, 1e-5, dev)
	hidden, _ := nn.NewLinear(cfg.Dim, cfg.FFNHiddenDim, true, dev)
	output, _ := nn.NewLinear(cfg.FFNHiddenDim, cfg.VocabSize, false, dev)

	return &SimpleLM{
		Embed: emb, Norm: norm, Hidden: hidden, Output: output,
		Config: cfg, Act: "gelu",
	}, nil
}

func (m *SimpleLM) Parameters() []*tensor.Tensor {
	var p []*tensor.Tensor
	p = append(p, m.Embed.Parameters()...)
	p = append(p, m.Norm.Parameters()...)
	p = append(p, m.Hidden.Parameters()...)
	p = append(p, m.Output.Parameters()...)
	return p
}

func (m *SimpleLM) CountParams() int {
	total := 0
	for _, p := range m.Parameters() {
		total += p.NumElements()
	}
	return total
}

// ForwardAndBackward does forward + backward manually with verified gradients.
// Returns loss value.
func (m *SimpleLM) ForwardAndBackward(inputs, targets *tensor.Tensor) (float64, error) {
	seqLen := inputs.Shape()[1]
	dim := m.Config.Dim

	// ---- Forward ----
	
	// 1. Embedding: [1, seqLen] → [seqLen, dim]
	tokens1D, _ := tensor.FromSlice(inputs.ToInt64Slice(), tensor.Shape{seqLen})
	emb, _ := m.Embed.Forward(tokens1D)
	
	// Reshape to [1, seqLen, dim] for 3D ops
	embData := emb.ToFloat32Slice()
	emb3D, _ := tensor.FromSlice(embData, tensor.Shape{1, seqLen, dim})
	
	// 2. LayerNorm
	normed, _ := m.Norm.Forward(emb3D)
	
	// 3. Hidden linear: [1, seqLen, dim] → [1, seqLen, hiddenDim]
	// Flatten to [seqLen, dim] for linear
	normedFlat, _ := tensor.FromSlice(normed.ToFloat32Slice(), tensor.Shape{seqLen, dim})
	hiddenOut, _ := m.Hidden.Forward(normedFlat)
	
	// 4. GELU activation
	hiddenAct := geluFwd(hiddenOut)
	
	// 5. Output: [seqLen, hiddenDim] → [seqLen, vocabSize]
	logitsFlat, _ := m.Output.Forward(hiddenAct)
	
	// Reshape to [1, seqLen, vocabSize]
	logits, _ := tensor.FromSlice(logitsFlat.ToFloat32Slice(), 
		tensor.Shape{1, seqLen, m.Config.VocabSize})
	
	// 6. Cross-entropy loss
	loss, _ := ops.CrossEntropyLoss(logits, targets)
	lossVal := float64(loss.ToFloat32Slice()[0])
	
	// ---- Backward ----
	
	// dLogits: [1, seqLen, vocabSize]
	dLogits, _ := ops.CrossEntropyBackward(logits, targets)
	
	// Flatten to [seqLen, vocabSize]
	dLogitsFlat, _ := tensor.FromSlice(dLogits.ToFloat32Slice(),
		tensor.Shape{seqLen, m.Config.VocabSize})
	
	// Backward through Output: dHiddenAct [seqLen, hiddenDim]
	dHiddenAct, _ := m.Output.Backward(hiddenAct, dLogitsFlat)
	
	// Backward through GELU
	dHidden := geluBwd(hiddenOut, dHiddenAct)
	
	// Backward through Hidden linear: dNormedFlat [seqLen, dim]
	dNormedFlat, _ := m.Hidden.Backward(normedFlat, dHidden)
	
	// Reshape back to [1, seqLen, dim] for LayerNorm backward
	dNormed3D, _ := tensor.FromSlice(dNormedFlat.ToFloat32Slice(),
		tensor.Shape{1, seqLen, dim})
	
	// Backward through LayerNorm → gives gradient w.r.t embedding output
	dEmb3D, _ := m.Norm.Backward(emb3D, dNormed3D)
	
	// Backward through Embedding
	dEmb2D, _ := tensor.FromSlice(dEmb3D.ToFloat32Slice(),
		tensor.Shape{seqLen, dim})
	m.Embed.Backward(tokens1D, dEmb2D)
	
	return lossVal, nil
}

func geluFwd(x *tensor.Tensor) *tensor.Tensor {
	data := x.ToFloat32Slice()
	c := float32(math.Sqrt(2.0 / math.Pi))
	out := make([]float32, len(data))
	for i, v := range data {
		out[i] = 0.5 * v * (1 + float32(math.Tanh(float64(c*(v+0.044715*v*v*v)))))
	}
	t, _ := tensor.FromSlice(out, x.Shape())
	return t
}

func geluBwd(x, dout *tensor.Tensor) *tensor.Tensor {
	xData := x.ToFloat32Slice()
	dData := dout.ToFloat32Slice()
	c := float64(math.Sqrt(2.0 / math.Pi))
	out := make([]float32, len(xData))
	for i, v := range xData {
		vf := float64(v)
		inner := c * (vf + 0.044715*vf*vf*vf)
		tanh := math.Tanh(inner)
		dtanh := 1 - tanh*tanh
		dinner := c * (1 + 3*0.044715*vf*vf)
		grad := 0.5*(1+tanh) + 0.5*vf*dtanh*dinner
		out[i] = dData[i] * float32(grad)
	}
	t, _ := tensor.FromSlice(out, x.Shape())
	return t
}

func main() {
	fmt.Println("=== GoML — SimpleLM Training ===\n")

	data, err := os.ReadFile("data/shakespeare.txt")
	if err != nil {
		data = []byte("To be, or not to be, that is the question:\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune,\nOr to take arms against a sea of troubles,\n")
	}

	tok := tokenizer.NewByteTokenizer()
	allTokens := tok.Encode(string(data))
	splitIdx := int(float64(len(allTokens)) * 0.9)
	trainTokens := allTokens[:splitIdx]
	evalTokens := allTokens[splitIdx:]

	fmt.Printf("Data: %d tokens (train: %d, eval: %d)\n", len(allTokens), len(trainTokens), len(evalTokens))

	cfg := nn.ModelConfig{
		VocabSize: 256, Dim: 64, NumLayers: 0, NumHeads: 0,
		FFNHiddenDim: 256, MaxSeqLen: 64, NormEps: 1e-5,
	}

	model, _ := NewSimpleLM(cfg, backend.CPU0)
	fmt.Printf("Model: %d params\n", model.CountParams())

	// AdamW manually
	params := model.Parameters()
	lr := 1e-3
	beta1, beta2 := 0.9, 0.999
	eps := 1e-8
	wd := 0.01

	m := make([][]float32, len(params))
	v := make([][]float32, len(params))
	for i, p := range params {
		n := p.NumElements()
		m[i] = make([]float32, n)
		v[i] = make([]float32, n)
	}

	seqLen := 32
	steps := 2000

	fmt.Printf("Config: seqLen=%d, lr=%.0e, steps=%d\n\n", seqLen, lr, steps)

	// ---- Gradient check first ----
	fmt.Println("--- Gradient Check ---")
	{
		inp, _ := tensor.FromSlice([]int64{72, 101, 108, 108}, tensor.Shape{1, 4})
		tgt, _ := tensor.FromSlice([]int64{101, 108, 108, 111}, tensor.Shape{1, 4})

		// Zero grads
		for _, p := range params {
			p.SetGrad(nil)
		}

		lossBase, _ := model.ForwardAndBackward(inp, tgt)
		fmt.Printf("Base loss: %.4f (expected ~%.4f)\n", lossBase, math.Log(256))

		epsilon := 1e-4
		for pi, p := range params {
			if p.Grad() == nil {
				continue
			}
			pData := p.ToFloat32Slice()
			gData := p.Grad().ToFloat32Slice()

			// Check first element
			orig := pData[0]

			pData[0] = orig + float32(epsilon)
			// Need fresh forward without backward
			inp2, _ := tensor.FromSlice([]int64{72, 101, 108, 108}, tensor.Shape{1, 4})
			tgt2, _ := tensor.FromSlice([]int64{101, 108, 108, 111}, tensor.Shape{1, 4})
			for _, pp := range params { pp.SetGrad(nil) }
			lPlus, _ := model.ForwardAndBackward(inp2, tgt2)

			pData[0] = orig - float32(epsilon)
			inp3, _ := tensor.FromSlice([]int64{72, 101, 108, 108}, tensor.Shape{1, 4})
			tgt3, _ := tensor.FromSlice([]int64{101, 108, 108, 111}, tensor.Shape{1, 4})
			for _, pp := range params { pp.SetGrad(nil) }
			lMinus, _ := model.ForwardAndBackward(inp3, tgt3)

			pData[0] = orig

			numGrad := (lPlus - lMinus) / (2 * epsilon)

			// Restore original grad
			for _, pp := range params { pp.SetGrad(nil) }
			model.ForwardAndBackward(inp, tgt)

			anaGrad := float64(gData[0])
			relErr := math.Abs(numGrad-anaGrad) / (math.Abs(numGrad) + math.Abs(anaGrad) + 1e-8)

			status := "✓"
			if relErr > 0.01 { status = "✗" }
			fmt.Printf("  param %d: ana=%.6f num=%.6f rel_err=%.6f %s\n", pi, anaGrad, numGrad, relErr, status)
			if pi >= 5 { break } // check first few
		}
	}

	// ---- Training ----
	fmt.Println("\n--- Training ---")
	totalStart := time.Now()
	smoothLoss := float64(0)

	for step := 1; step <= steps; step++ {
		// Zero grads
		for _, p := range params {
			p.SetGrad(nil)
		}

		// Get random sequence
		maxStart := len(trainTokens) - seqLen - 1
		start := rand.Intn(maxStart)
		inputData := trainTokens[start : start+seqLen]
		targetData := trainTokens[start+1 : start+seqLen+1]
		inputs, _ := tensor.FromSlice(inputData, tensor.Shape{1, seqLen})
		targets, _ := tensor.FromSlice(targetData, tensor.Shape{1, seqLen})

		// Forward + backward
		lossVal, err := model.ForwardAndBackward(inputs, targets)
		if err != nil || math.IsNaN(lossVal) {
			fmt.Printf("step %d: error or NaN\n", step)
			continue
		}

		if smoothLoss == 0 {
			smoothLoss = lossVal
		} else {
			smoothLoss = 0.99*smoothLoss + 0.01*lossVal
		}

		// LR schedule
		warmup := 100
		currentLR := lr
		if step < warmup {
			currentLR = lr * float64(step) / float64(warmup)
		} else {
			progress := float64(step-warmup) / float64(steps-warmup)
			currentLR = lr*0.1 + 0.5*(lr-lr*0.1)*(1+math.Cos(math.Pi*progress))
		}

		// AdamW step
		t := float64(step)
		bc1 := 1.0 - math.Pow(beta1, t)
		bc2 := 1.0 - math.Pow(beta2, t)

		for i, p := range params {
			grad := p.Grad()
			if grad == nil { continue }
			pData := p.ToFloat32Slice()
			gData := grad.ToFloat32Slice()

			for j := range pData {
				g := gData[j]
				m[i][j] = float32(beta1)*m[i][j] + float32(1-beta1)*g
				v[i][j] = float32(beta2)*v[i][j] + float32(1-beta2)*g*g
				mHat := float64(m[i][j]) / bc1
				vHat := float64(v[i][j]) / bc2
				update := mHat / (math.Sqrt(vHat) + eps)
				pData[j] -= float32(currentLR) * (float32(update) + float32(wd)*pData[j])
			}
		}

		if step%100 == 0 || step == 1 {
			tokSec := float64(seqLen) / time.Since(totalStart).Seconds() * float64(step)
			_ = tokSec
			fmt.Printf("step %4d | loss %.4f (smooth %.4f) | lr %.1e\n",
				step, lossVal, smoothLoss, currentLR)
		}
	}

	totalTime := time.Since(totalStart)
	fmt.Printf("\nDone in %v\n", totalTime)
	fmt.Printf("Final smooth loss: %.4f (random: %.4f)\n", smoothLoss, math.Log(256))

	// Generate
	fmt.Println("\n--- Generation ---")
	for _, prompt := range []string{"The ", "To be", "KING "} {
		tokens := tok.Encode(prompt)
		for i := 0; i < 100; i++ {
			window := tokens
			if len(window) > cfg.MaxSeqLen {
				window = window[len(window)-cfg.MaxSeqLen:]
			}
			input, _ := tensor.FromSlice(window, tensor.Shape{1, len(window)})

			// Just forward, no backward
			seqL := len(window)
			tokens1D, _ := tensor.FromSlice(input.ToInt64Slice(), tensor.Shape{seqL})
			emb, _ := model.Embed.Forward(tokens1D)
			embData := emb.ToFloat32Slice()
			emb3D, _ := tensor.FromSlice(embData, tensor.Shape{1, seqL, cfg.Dim})
			normed, _ := model.Norm.Forward(emb3D)
			normedFlat, _ := tensor.FromSlice(normed.ToFloat32Slice(), tensor.Shape{seqL, cfg.Dim})
			hiddenOut, _ := model.Hidden.Forward(normedFlat)
			hiddenAct := geluFwd(hiddenOut)
			logitsFlat, _ := model.Output.Forward(hiddenAct)

			logitsData := logitsFlat.ToFloat32Slice()
			lastOff := (seqL - 1) * cfg.VocabSize
			lastLogits := logitsData[lastOff : lastOff+cfg.VocabSize]

			// Temperature sampling
			temp := float32(0.8)
			for k := range lastLogits { lastLogits[k] /= temp }

			// Softmax
			maxV := float32(-1e9)
			for _, v := range lastLogits { if v > maxV { maxV = v } }
			sumExp := float32(0)
			for k := range lastLogits {
				lastLogits[k] = float32(math.Exp(float64(lastLogits[k] - maxV)))
				sumExp += lastLogits[k]
			}
			for k := range lastLogits { lastLogits[k] /= sumExp }

			// Sample
			r := rand.Float32()
			cum := float32(0)
			next := int64(0)
			for k, p := range lastLogits {
				cum += p
				if r < cum { next = int64(k); break }
			}
			tokens = append(tokens, next)
		}
		fmt.Printf("%q → %q\n\n", prompt, tok.Decode(tokens))
	}
}
```

## File: ./core/dtype.go
```go
package core

import (
	"fmt"
	"unsafe"
)

// DType represents the data type of tensor elements.
type DType uint8

const (
	Float16 DType = iota
	Float32
	Float64
	BFloat16
	Int8
	Int16
	Int32
	Int64
	Uint8
	Bool
)

// Size returns the byte size of one element.
func (d DType) Size() uintptr {
	switch d {
	case Float16, BFloat16, Int16:
		return 2
	case Float32, Int32:
		return 4
	case Float64, Int64:
		return 8
	case Int8, Uint8, Bool:
		return 1
	default:
		panic(fmt.Sprintf("unknown dtype: %d", d))
	}
}

func (d DType) String() string {
	names := [...]string{
		"float16", "float32", "float64", "bfloat16",
		"int8", "int16", "int32", "int64", "uint8", "bool",
	}
	if int(d) < len(names) {
		return names[d]
	}
	return fmt.Sprintf("dtype(%d)", d)
}

// IsFloat returns true for floating point types.
func (d DType) IsFloat() bool {
	return d == Float16 || d == Float32 || d == Float64 || d == BFloat16
}

// BFloat16Value represents a bfloat16 number (brain floating point).
// Stored as uint16, upper 16 bits of float32.
type BFloat16Value uint16

func BFloat16FromFloat32(f float32) BFloat16Value {
	bits := *(*uint32)(unsafe.Pointer(&f))
	return BFloat16Value(bits >> 16)
}

func (b BFloat16Value) Float32() float32 {
	bits := uint32(b) << 16
	return *(*float32)(unsafe.Pointer(&bits))
}
```

## File: ./core/shape.go
```go
package core

import "fmt"

// Shape represents the dimensions of a tensor.
type Shape []int

// Strides represents byte offsets between consecutive elements along each dimension.
type Strides []int

// NumElements returns the total number of elements in the shape.
func (s Shape) NumElements() int {
	if len(s) == 0 {
		return 1 // scalar
	}
	n := 1
	for _, d := range s {
		n *= d
	}
	return n
}

// NDim returns the number of dimensions.
func (s Shape) NDim() int {
	return len(s)
}

// Equal checks if two shapes are identical.
func (s Shape) Equal(other Shape) bool {
	if len(s) != len(other) {
		return false
	}
	for i := range s {
		if s[i] != other[i] {
			return false
		}
	}
	return true
}

// Clone returns a copy of the shape.
func (s Shape) Clone() Shape {
	c := make(Shape, len(s))
	copy(c, s)
	return c
}

func (s Shape) String() string {
	return fmt.Sprintf("%v", []int(s))
}

// ContiguousStrides computes row-major (C-order) strides for a given shape and element size.
func ContiguousStrides(shape Shape, elemSize uintptr) Strides {
	ndim := len(shape)
	if ndim == 0 {
		return Strides{}
	}
	strides := make(Strides, ndim)
	strides[ndim-1] = int(elemSize)
	for i := ndim - 2; i >= 0; i-- {
		strides[i] = strides[i+1] * shape[i+1]
	}
	return strides
}

// IsContiguous checks if strides represent a contiguous row-major layout.
func IsContiguous(shape Shape, strides Strides, elemSize uintptr) bool {
	if len(shape) != len(strides) {
		return false
	}
	expected := ContiguousStrides(shape, elemSize)
	for i := range strides {
		if strides[i] != expected[i] {
			return false
		}
	}
	return true
}

// BroadcastShapes returns the broadcast-compatible shape of two shapes.
// Follows NumPy broadcasting rules.
func BroadcastShapes(a, b Shape) (Shape, error) {
	maxDim := len(a)
	if len(b) > maxDim {
		maxDim = len(b)
	}

	result := make(Shape, maxDim)
	for i := 0; i < maxDim; i++ {
		da := 1
		db := 1
		if i < len(a) {
			da = a[len(a)-1-i]
		}
		if i < len(b) {
			db = b[len(b)-1-i]
		}

		switch {
		case da == db:
			result[maxDim-1-i] = da
		case da == 1:
			result[maxDim-1-i] = db
		case db == 1:
			result[maxDim-1-i] = da
		default:
			return nil, fmt.Errorf("shapes %v and %v are not broadcast-compatible", a, b)
		}
	}
	return result, nil
}

// FlatIndex converts a multi-dimensional index to a flat byte offset.
func FlatIndex(indices []int, strides Strides) int {
	offset := 0
	for i, idx := range indices {
		offset += idx * strides[i]
	}
	return offset
}

// Permute returns new shape and strides for a transposed view.
func Permute(shape Shape, strides Strides, axes []int) (Shape, Strides, error) {
	if len(axes) != len(shape) {
		return nil, nil, fmt.Errorf("axes length %d != ndim %d", len(axes), len(shape))
	}

	seen := make([]bool, len(axes))
	for _, a := range axes {
		if a < 0 || a >= len(shape) {
			return nil, nil, fmt.Errorf("axis %d out of range for %d dimensions", a, len(shape))
		}
		if seen[a] {
			return nil, nil, fmt.Errorf("duplicate axis %d", a)
		}
		seen[a] = true
	}

	newShape := make(Shape, len(shape))
	newStrides := make(Strides, len(strides))
	for i, a := range axes {
		newShape[i] = shape[a]
		newStrides[i] = strides[a]
	}
	return newShape, newStrides, nil
}
```

## File: ./main.go
```go
package main

import (
	"fmt"
	"math"
	"math/rand"
	"os"
	"time"

	_ "github.com/vugar/goml/backend/cpu"
	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/nn"
	"github.com/vugar/goml/ops"
	"github.com/vugar/goml/optim"
	"github.com/vugar/goml/tensor"
	"github.com/vugar/goml/tokenizer"
)

func main() {
	fmt.Println("=== GoML — LLM Training from Scratch ===\n")

	// Load data
	data, err := os.ReadFile("data/shakespeare.txt")
	if err != nil {
		fmt.Println("No data file, using inline text")
		data = []byte(inlineText)
	}

	tok := tokenizer.NewByteTokenizer()
	allTokens := tok.Encode(string(data))
	
	// 90/10 split
	splitIdx := int(float64(len(allTokens)) * 0.9)
	trainTokens := allTokens[:splitIdx]
	evalTokens := allTokens[splitIdx:]
	
	fmt.Printf("Data: %d tokens (train: %d, eval: %d)\n", len(allTokens), len(trainTokens), len(evalTokens))

	// Model
	cfg := nn.TinyConfig()
	cfg.MaxSeqLen = 64
	
	model, _ := nn.NewLLM(cfg, backend.CPU0)
	totalParams := model.CountParameters()
	fmt.Printf("Model: %d params (%.2f K)\n", totalParams, float64(totalParams)/1e3)

	// Optimizer
	opt := optim.NewAdamW(model.Parameters(), 3e-4)

	// Training config
	seqLen := 32
	batchSize := 1
	steps := 3000
	logEvery := 200
	evalEvery := 1000
	genEvery := 1000

	fmt.Printf("Config: batch=%d, seqLen=%d, lr=3e-4, steps=%d\n\n", batchSize, seqLen, steps)

	totalStart := time.Now()
	smoothLoss := float64(0)
	bestEval := math.MaxFloat64

	for step := 1; step <= steps; step++ {
		stepStart := time.Now()

		// Cosine LR schedule with warmup
		lr := optim.CosineSchedule(step, 200, steps, 3e-4, 3e-5)
		opt.SetLR(lr)

		// Get batch
		inputs, targets := getBatch(trainTokens, batchSize, seqLen)

		// Forward
		logits, cache, err := model.ForwardWithCache(inputs)
		if err != nil {
			fmt.Printf("step %d: forward err: %v\n", step, err)
			continue
		}

		// Check for NaN
		if hasNaN(logits.ToFloat32Slice()) {
			fmt.Printf("step %d: NaN in logits\n", step)
			continue
		}

		// Loss
		loss, _ := ops.CrossEntropyLoss(logits, targets)
		lossVal := float64(loss.ToFloat32Slice()[0])

		if math.IsNaN(lossVal) || math.IsInf(lossVal, 0) {
			fmt.Printf("step %d: bad loss %.4f\n", step, lossVal)
			continue
		}

		if smoothLoss == 0 {
			smoothLoss = lossVal
		} else {
			smoothLoss = 0.99*smoothLoss + 0.01*lossVal
		}

		// Backward
		opt.ZeroGrad()
		dLogits, _ := ops.CrossEntropyBackward(logits, targets)
		err = model.Backward(cache, dLogits)
		if err != nil {
			fmt.Printf("step %d: backward err: %v\n", step, err)
			continue
		}

		// Optimizer step
		opt.Step()

		elapsed := time.Since(stepStart)
		tokSec := float64(batchSize*seqLen) / elapsed.Seconds()

		if step%logEvery == 0 || step == 1 {
			fmt.Printf("step %4d | loss %.4f (smooth %.4f) | lr %.1e | %.0f tok/s | %v\n",
				step, lossVal, smoothLoss, lr, tokSec, elapsed)
		}

		if step%evalEvery == 0 {
			evalLoss := evaluate(model, evalTokens, seqLen)
			tag := ""
			if evalLoss < bestEval {
				bestEval = evalLoss
				tag = " *best"
			}
			fmt.Printf("         → eval loss: %.4f%s\n", evalLoss, tag)
		}

		if step%genEvery == 0 {
			sample := generate(model, tok, cfg, "The ", 100, 0.8)
			fmt.Printf("         → sample: %q\n", truncate(sample, 120))
		}
	}

	totalTime := time.Since(totalStart)
	fmt.Printf("\nTraining complete in %v\n", totalTime)
	fmt.Printf("Best eval loss: %.4f (random baseline: %.4f)\n", bestEval, math.Log(float64(cfg.VocabSize)))

	// Final generation
	fmt.Println("\n--- Final Generation ---")
	prompts := []string{"KING ", "To be ", "The ", "What "}
	for _, p := range prompts {
		sample := generate(model, tok, cfg, p, 200, 0.7)
		fmt.Printf("\n%q → %s\n", p, truncate(sample, 200))
	}
}

func getBatch(tokens []int64, batchSize, seqLen int) (*tensor.Tensor, *tensor.Tensor) {
	maxStart := len(tokens) - seqLen - 1
	inputData := make([]int64, batchSize*seqLen)
	targetData := make([]int64, batchSize*seqLen)

	for b := 0; b < batchSize; b++ {
		start := rand.Intn(maxStart)
		for s := 0; s < seqLen; s++ {
			inputData[b*seqLen+s] = tokens[start+s]
			targetData[b*seqLen+s] = tokens[start+s+1]
		}
	}

	inputs, _ := tensor.FromSlice(inputData, tensor.Shape{batchSize, seqLen})
	targets, _ := tensor.FromSlice(targetData, tensor.Shape{batchSize, seqLen})
	return inputs, targets
}

func evaluate(model *nn.LLM, tokens []int64, seqLen int) float64 {
	totalLoss := float64(0)
	numBatches := 10
	for i := 0; i < numBatches; i++ {
		inputs, targets := getBatch(tokens, 1, seqLen)
		logits, _ := model.Forward(inputs)
		loss, _ := ops.CrossEntropyLoss(logits, targets)
		totalLoss += float64(loss.ToFloat32Slice()[0])
	}
	return totalLoss / float64(numBatches)
}

func generate(model *nn.LLM, tok *tokenizer.ByteTokenizer, cfg nn.ModelConfig, prompt string, maxLen int, temperature float32) string {
	tokens := tok.Encode(prompt)
	for i := 0; i < maxLen; i++ {
		start := 0
		if len(tokens) > cfg.MaxSeqLen {
			start = len(tokens) - cfg.MaxSeqLen
		}
		window := tokens[start:]
		input, _ := tensor.FromSlice(window, tensor.Shape{1, len(window)})
		logits, _ := model.Forward(input)
		data := logits.ToFloat32Slice()
		lastOff := (len(window) - 1) * cfg.VocabSize
		last, _ := tensor.FromSlice(data[lastOff:lastOff+cfg.VocabSize], tensor.Shape{cfg.VocabSize})
		next := int64(nn.TopKSample(last, 40, temperature))
		tokens = append(tokens, next)
	}
	return tok.Decode(tokens)
}

func hasNaN(data []float32) bool {
	for _, v := range data {
		if math.IsNaN(float64(v)) || math.IsInf(float64(v), 0) {
			return true
		}
	}
	return false
}

func truncate(s string, maxLen int) string {
	if len(s) > maxLen {
		return s[:maxLen] + "..."
	}
	return s
}

const inlineText = `KING HENRY:
Once more unto the breach, dear friends, once more;
Or close the wall up with our English dead.
In peace there's nothing so becomes a man
As modest stillness and humility.`
```

## File: ./nn/attention.go
```go
package nn

import (
	"fmt"

	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/ops"
	"github.com/vugar/goml/tensor"
)

// MultiHeadAttention implements multi-head self-attention.
// Architecture follows LLaMA style: separate Q, K, V projections, no output bias.
type MultiHeadAttention struct {
	Wq *Linear // [dim, dim]
	Wk *Linear // [dim, dim]
	Wv *Linear // [dim, dim]
	Wo *Linear // [dim, dim]

	NumHeads int
	HeadDim  int
	Dim      int

	// KV Cache for inference
	CacheK *tensor.Tensor // [batch, numHeads, cachedLen, headDim]
	CacheV *tensor.Tensor // [batch, numHeads, cachedLen, headDim]
}

// NewMultiHeadAttention creates an MHA layer.
func NewMultiHeadAttention(dim, numHeads int, device backend.Device) (*MultiHeadAttention, error) {
	if dim%numHeads != 0 {
		return nil, fmt.Errorf("dim %d not divisible by numHeads %d", dim, numHeads)
	}
	headDim := dim / numHeads

	wq, err := NewLinear(dim, dim, false, device)
	if err != nil {
		return nil, err
	}
	wk, err := NewLinear(dim, dim, false, device)
	if err != nil {
		return nil, err
	}
	wv, err := NewLinear(dim, dim, false, device)
	if err != nil {
		return nil, err
	}
	wo, err := NewLinear(dim, dim, false, device)
	if err != nil {
		return nil, err
	}

	return &MultiHeadAttention{
		Wq: wq, Wk: wk, Wv: wv, Wo: wo,
		NumHeads: numHeads, HeadDim: headDim, Dim: dim,
	}, nil
}

// Forward runs multi-head attention.
// x shape: [batch, seqLen, dim] → [batch, seqLen, dim]
func (mha *MultiHeadAttention) Forward(x *tensor.Tensor, causal bool) (*tensor.Tensor, error) {
	shape := x.Shape()
	batch := shape[0]
	seqLen := shape[1]

	// Project Q, K, V: [batch, seqLen, dim]
	q, err := mha.Wq.Forward(x)
	if err != nil {
		return nil, fmt.Errorf("Wq: %w", err)
	}
	k, err := mha.Wk.Forward(x)
	if err != nil {
		return nil, fmt.Errorf("Wk: %w", err)
	}
	v, err := mha.Wv.Forward(x)
	if err != nil {
		return nil, fmt.Errorf("Wv: %w", err)
	}

	// Reshape to [batch, seqLen, numHeads, headDim]
	q, err = q.View(tensor.Shape{batch, seqLen, mha.NumHeads, mha.HeadDim})
	if err != nil {
		return nil, err
	}
	k, err = k.View(tensor.Shape{batch, seqLen, mha.NumHeads, mha.HeadDim})
	if err != nil {
		return nil, err
	}
	v, err = v.View(tensor.Shape{batch, seqLen, mha.NumHeads, mha.HeadDim})
	if err != nil {
		return nil, err
	}

	// Transpose to [batch, numHeads, seqLen, headDim]
	q, err = q.Transpose([]int{0, 2, 1, 3})
	if err != nil {
		return nil, err
	}
	k, err = k.Transpose([]int{0, 2, 1, 3})
	if err != nil {
		return nil, err
	}
	v, err = v.Transpose([]int{0, 2, 1, 3})
	if err != nil {
		return nil, err
	}

	// Apply RoPE to Q and K
	q, err = applyRoPE(q, mha.HeadDim)
	if err != nil {
		return nil, fmt.Errorf("rope q: %w", err)
	}
	k, err = applyRoPE(k, mha.HeadDim)
	if err != nil {
		return nil, fmt.Errorf("rope k: %w", err)
	}

	// Make Q, K, V contiguous for attention kernel
	q, err = makeContiguous(q)
	if err != nil {
		return nil, err
	}
	k, err = makeContiguous(k)
	if err != nil {
		return nil, err
	}
	v, err = makeContiguous(v)
	if err != nil {
		return nil, err
	}

	// Scaled dot-product attention
	attnOut, err := ops.ScaledDotProductAttention(q, k, v, mha.NumHeads, causal)
	if err != nil {
		return nil, fmt.Errorf("attention: %w", err)
	}

	// Transpose back: [batch, numHeads, seqLen, headDim] → [batch, seqLen, numHeads, headDim]
	attnOut, err = attnOut.Transpose([]int{0, 2, 1, 3})
	if err != nil {
		return nil, err
	}

	// Make contiguous and reshape to [batch, seqLen, dim]
	attnOut, err = makeContiguous(attnOut)
	if err != nil {
		return nil, err
	}
	attnOut, err = attnOut.View(tensor.Shape{batch, seqLen, mha.Dim})
	if err != nil {
		return nil, err
	}

	// Output projection
	out, err := mha.Wo.Forward(attnOut)
	if err != nil {
		return nil, fmt.Errorf("Wo: %w", err)
	}

	return out, nil
}

// Parameters returns all trainable parameters.
func (mha *MultiHeadAttention) Parameters() []*tensor.Tensor {
	var params []*tensor.Tensor
	params = append(params, mha.Wq.Parameters()...)
	params = append(params, mha.Wk.Parameters()...)
	params = append(params, mha.Wv.Parameters()...)
	params = append(params, mha.Wo.Parameters()...)
	return params
}

// applyRoPE applies rotary positional embeddings.
func applyRoPE(x *tensor.Tensor, headDim int) (*tensor.Tensor, error) {
	bk, err := backend.GetForDevice(x.Device())
	if err != nil {
		return nil, err
	}

	store, err := bk.Alloc(x.NumElements() * int(x.DType().Size()))
	if err != nil {
		return nil, err
	}

	err = bk.RoPE(store, x.Storage(), x.Shape(), headDim, 10000.0, x.DType())
	if err != nil {
		store.Free()
		return nil, err
	}

	return tensor.NewTensor(store, x.Shape(), x.DType()), nil
}

// makeContiguous copies a non-contiguous tensor into contiguous memory.
func makeContiguous(t *tensor.Tensor) (*tensor.Tensor, error) {
	if t.IsContiguous() {
		return t, nil
	}

	bk, err := backend.GetForDevice(t.Device())
	if err != nil {
		return nil, err
	}

	n := t.NumElements()
	byteLen := n * int(t.DType().Size())
	store, err := bk.Alloc(byteLen)
	if err != nil {
		return nil, err
	}

	// Copy element by element following strides
	shape := t.Shape()
	strides := t.Strides()
	ndim := len(shape)
	indices := make([]int, ndim)

	srcBytes := t.Storage().Bytes()
	dstBytes := store.Bytes()

	elemSize := int(t.DType().Size())

	for i := 0; i < n; i++ {
		// Compute source byte offset from strides
		srcOffset := 0
		for d := 0; d < ndim; d++ {
			srcOffset += indices[d] * strides[d]
		}

		// Copy one element
		dstOff := i * elemSize
		copy(dstBytes[dstOff:dstOff+elemSize], srcBytes[srcOffset:srcOffset+elemSize])

		// Increment indices
		for d := ndim - 1; d >= 0; d-- {
			indices[d]++
			if indices[d] < shape[d] {
				break
			}
			indices[d] = 0
		}
	}

	return tensor.NewTensor(store, shape, t.DType()), nil
}
```

## File: ./nn/backward_attn.go
```go
package nn

import (
	"math"

	"github.com/vugar/goml/tensor"
)

// ---- Attention Backward ----

// AttentionCache stores intermediate values needed for backward pass.
type AttentionCache struct {
	X       *tensor.Tensor // input
	Q, K, V *tensor.Tensor // after projection, shape [batch, heads, seq, headDim]
	Scores  []float32      // attention weights after softmax
}

// ForwardWithCache runs attention and saves intermediates for backward.
func (mha *MultiHeadAttention) ForwardWithCache(x *tensor.Tensor, causal bool) (*tensor.Tensor, *AttentionCache, error) {
	shape := x.Shape()
	batch := shape[0]
	seqLen := shape[1]

	// Project Q, K, V
	q, _ := mha.Wq.Forward(x)
	k, _ := mha.Wk.Forward(x)
	v, _ := mha.Wv.Forward(x)

	// Reshape to [batch*seqLen, numHeads, headDim] then rearrange
	qFlat := q.ToFloat32Slice()
	kFlat := k.ToFloat32Slice()
	vFlat := v.ToFloat32Slice()

	// Rearrange to [batch, heads, seq, headDim]
	qArr := rearrangeBSHD(qFlat, batch, seqLen, mha.NumHeads, mha.HeadDim)
	kArr := rearrangeBSHD(kFlat, batch, seqLen, mha.NumHeads, mha.HeadDim)
	vArr := rearrangeBSHD(vFlat, batch, seqLen, mha.NumHeads, mha.HeadDim)

	// Apply RoPE
	applyRoPEInPlace(qArr, batch, mha.NumHeads, seqLen, mha.HeadDim, 10000.0)
	applyRoPEInPlace(kArr, batch, mha.NumHeads, seqLen, mha.HeadDim, 10000.0)

	// Attention: scores = Q @ K^T / sqrt(d)
	scale := float32(1.0 / math.Sqrt(float64(mha.HeadDim)))
	allScores := make([]float32, batch*mha.NumHeads*seqLen*seqLen)
	outArr := make([]float32, batch*mha.NumHeads*seqLen*mha.HeadDim)

	for b := 0; b < batch; b++ {
		for h := 0; h < mha.NumHeads; h++ {
			bhOff := (b*mha.NumHeads + h) * seqLen * mha.HeadDim
			scOff := (b*mha.NumHeads + h) * seqLen * seqLen

			// Q @ K^T
			for i := 0; i < seqLen; i++ {
				for j := 0; j < seqLen; j++ {
					dot := float32(0)
					for d := 0; d < mha.HeadDim; d++ {
						dot += qArr[bhOff+i*mha.HeadDim+d] * kArr[bhOff+j*mha.HeadDim+d]
					}
					allScores[scOff+i*seqLen+j] = dot * scale
					if causal && j > i {
						allScores[scOff+i*seqLen+j] = -1e9
					}
				}
			}

			// Softmax per row
			for i := 0; i < seqLen; i++ {
				maxVal := float32(-math.MaxFloat32)
				for j := 0; j < seqLen; j++ {
					if allScores[scOff+i*seqLen+j] > maxVal {
						maxVal = allScores[scOff+i*seqLen+j]
					}
				}
				sumExp := float32(0)
				for j := 0; j < seqLen; j++ {
					allScores[scOff+i*seqLen+j] = float32(math.Exp(float64(allScores[scOff+i*seqLen+j] - maxVal)))
					sumExp += allScores[scOff+i*seqLen+j]
				}
				for j := 0; j < seqLen; j++ {
					allScores[scOff+i*seqLen+j] /= sumExp
				}
			}

			// Attn @ V
			for i := 0; i < seqLen; i++ {
				for d := 0; d < mha.HeadDim; d++ {
					sum := float32(0)
					for j := 0; j < seqLen; j++ {
						sum += allScores[scOff+i*seqLen+j] * vArr[bhOff+j*mha.HeadDim+d]
					}
					outArr[bhOff+i*mha.HeadDim+d] = sum
				}
			}
		}
	}

	// Rearrange back to [batch, seq, dim]
	outFlat := rearrangeBHSD(outArr, batch, seqLen, mha.NumHeads, mha.HeadDim)

	outTensor, _ := tensor.FromSlice(outFlat, tensor.Shape{batch, seqLen, mha.Dim})

	// Output projection
	result, _ := mha.Wo.Forward(outTensor)

	// Save cache
	qT, _ := tensor.FromSlice(qArr, tensor.Shape{batch, mha.NumHeads, seqLen, mha.HeadDim})
	kT, _ := tensor.FromSlice(kArr, tensor.Shape{batch, mha.NumHeads, seqLen, mha.HeadDim})
	vT, _ := tensor.FromSlice(vArr, tensor.Shape{batch, mha.NumHeads, seqLen, mha.HeadDim})

	cache := &AttentionCache{X: x, Q: qT, K: kT, V: vT, Scores: allScores}

	return result, cache, nil
}

// Backward computes gradients for attention.
func (mha *MultiHeadAttention) Backward(cache *AttentionCache, dout *tensor.Tensor) (*tensor.Tensor, error) {
	shape := cache.X.Shape()
	batch := shape[0]
	seqLen := shape[1]

	// Backward through Wo
	// Need to recompute attn output for Wo backward
	// First, get the pre-Wo tensor
	qArr := cache.Q.ToFloat32Slice()
	kArr := cache.K.ToFloat32Slice()
	vArr := cache.V.ToFloat32Slice()
	scores := cache.Scores

	// Recompute attn output
	outArr := make([]float32, batch*mha.NumHeads*seqLen*mha.HeadDim)
	for b := 0; b < batch; b++ {
		for h := 0; h < mha.NumHeads; h++ {
			bhOff := (b*mha.NumHeads + h) * seqLen * mha.HeadDim
			scOff := (b*mha.NumHeads + h) * seqLen * seqLen
			for i := 0; i < seqLen; i++ {
				for d := 0; d < mha.HeadDim; d++ {
					sum := float32(0)
					for j := 0; j < seqLen; j++ {
						sum += scores[scOff+i*seqLen+j] * vArr[bhOff+j*mha.HeadDim+d]
					}
					outArr[bhOff+i*mha.HeadDim+d] = sum
				}
			}
		}
	}
	outFlat := rearrangeBHSD(outArr, batch, seqLen, mha.NumHeads, mha.HeadDim)
	attnOutTensor, _ := tensor.FromSlice(outFlat, tensor.Shape{batch, seqLen, mha.Dim})

	// dAttnOut from Wo backward
	dAttnOut, _ := mha.Wo.Backward(attnOutTensor, dout)
	dAttnOutData := dAttnOut.ToFloat32Slice()

	// Rearrange dAttnOut to [batch, heads, seq, headDim]
	dOutArr := rearrangeBSHD(dAttnOutData, batch, seqLen, mha.NumHeads, mha.HeadDim)

	// Backward through attention: dScores, dV, dQ, dK
	scale := float32(1.0 / math.Sqrt(float64(mha.HeadDim)))
	dQArr := make([]float32, len(qArr))
	dKArr := make([]float32, len(kArr))
	dVArr := make([]float32, len(vArr))

	for b := 0; b < batch; b++ {
		for h := 0; h < mha.NumHeads; h++ {
			bhOff := (b*mha.NumHeads + h) * seqLen * mha.HeadDim
			scOff := (b*mha.NumHeads + h) * seqLen * seqLen

			// dV = scores^T @ dOut
			for j := 0; j < seqLen; j++ {
				for d := 0; d < mha.HeadDim; d++ {
					sum := float32(0)
					for i := 0; i < seqLen; i++ {
						sum += scores[scOff+i*seqLen+j] * dOutArr[bhOff+i*mha.HeadDim+d]
					}
					dVArr[bhOff+j*mha.HeadDim+d] = sum
				}
			}

			// dScores = dOut @ V^T
			dScores := make([]float32, seqLen*seqLen)
			for i := 0; i < seqLen; i++ {
				for j := 0; j < seqLen; j++ {
					sum := float32(0)
					for d := 0; d < mha.HeadDim; d++ {
						sum += dOutArr[bhOff+i*mha.HeadDim+d] * vArr[bhOff+j*mha.HeadDim+d]
					}
					dScores[i*seqLen+j] = sum
				}
			}

			// Backward through softmax: dPre = scores * (dScores - sum(dScores * scores))
			for i := 0; i < seqLen; i++ {
				dot := float32(0)
				for j := 0; j < seqLen; j++ {
					dot += dScores[i*seqLen+j] * scores[scOff+i*seqLen+j]
				}
				for j := 0; j < seqLen; j++ {
					dScores[i*seqLen+j] = scores[scOff+i*seqLen+j] * (dScores[i*seqLen+j] - dot) * scale
				}
			}

			// dQ = dPre @ K
			for i := 0; i < seqLen; i++ {
				for d := 0; d < mha.HeadDim; d++ {
					sum := float32(0)
					for j := 0; j < seqLen; j++ {
						sum += dScores[i*seqLen+j] * kArr[bhOff+j*mha.HeadDim+d]
					}
					dQArr[bhOff+i*mha.HeadDim+d] = sum
				}
			}

			// dK = dPre^T @ Q
			for j := 0; j < seqLen; j++ {
				for d := 0; d < mha.HeadDim; d++ {
					sum := float32(0)
					for i := 0; i < seqLen; i++ {
						sum += dScores[i*seqLen+j] * qArr[bhOff+i*mha.HeadDim+d]
					}
					dKArr[bhOff+j*mha.HeadDim+d] = sum
				}
			}
		}
	}

	// RoPE backward (reverse rotation — same as forward but with -angle)
	ropeBackwardInPlace(dQArr, batch, mha.NumHeads, seqLen, mha.HeadDim, 10000.0)
	ropeBackwardInPlace(dKArr, batch, mha.NumHeads, seqLen, mha.HeadDim, 10000.0)

	// Rearrange back to [batch, seq, dim]
	dQFlat := rearrangeBHSD(dQArr, batch, seqLen, mha.NumHeads, mha.HeadDim)
	dKFlat := rearrangeBHSD(dKArr, batch, seqLen, mha.NumHeads, mha.HeadDim)
	dVFlat := rearrangeBHSD(dVArr, batch, seqLen, mha.NumHeads, mha.HeadDim)

	dQ, _ := tensor.FromSlice(dQFlat, tensor.Shape{batch, seqLen, mha.Dim})
	dK, _ := tensor.FromSlice(dKFlat, tensor.Shape{batch, seqLen, mha.Dim})
	dV, _ := tensor.FromSlice(dVFlat, tensor.Shape{batch, seqLen, mha.Dim})

	// Backward through Wq, Wk, Wv
	dx1, _ := mha.Wq.Backward(cache.X, dQ)
	dx2, _ := mha.Wk.Backward(cache.X, dK)
	dx3, _ := mha.Wv.Backward(cache.X, dV)

	// dx = dx1 + dx2 + dx3
	return addTensors(addTensors(dx1, dx2), dx3), nil
}

// ---- TransformerBlock Backward ----

// BlockCache stores intermediates for transformer block backward.
type BlockCache struct {
	X          *tensor.Tensor // input to block
	Normed1    *tensor.Tensor // after first layernorm
	AttnOut    *tensor.Tensor // after attention (before residual)
	PostAttn   *tensor.Tensor // after first residual
	Normed2    *tensor.Tensor // after second layernorm
	FFNOut     *tensor.Tensor // after FFN (before residual)
	AttnCache  *AttentionCache
}

// ForwardWithCache runs transformer block and saves intermediates.
func (tb *TransformerBlock) ForwardWithCache(x *tensor.Tensor) (*tensor.Tensor, *BlockCache, error) {
	cache := &BlockCache{X: x}

	normed1, _ := tb.AttnNorm.Forward(x)
	cache.Normed1 = normed1

	attnOut, attnCache, err := tb.Attn.ForwardWithCache(normed1, true)
	if err != nil {
		return nil, nil, err
	}
	cache.AttnOut = attnOut
	cache.AttnCache = attnCache

	postAttn := addTensors(x, attnOut)
	cache.PostAttn = postAttn

	normed2, _ := tb.FFNNorm.Forward(postAttn)
	cache.Normed2 = normed2

	ffnOut, _ := tb.FFN.Forward(normed2)
	cache.FFNOut = ffnOut

	out := addTensors(postAttn, ffnOut)
	return out, cache, nil
}

// Backward computes gradients for transformer block.
func (tb *TransformerBlock) Backward(cache *BlockCache, dout *tensor.Tensor) (*tensor.Tensor, error) {
	// dout flows through second residual
	dFFNOut := dout
	dPostAttn := dout // copy via residual

	// Backward through FFN
	dNormed2, _ := tb.FFN.Backward(cache.Normed2, dFFNOut)

	// Backward through FFNNorm
	dPostAttn2, _ := tb.FFNNorm.Backward(cache.PostAttn, dNormed2)
	dPostAttn = addTensors(dPostAttn, dPostAttn2)

	// dPostAttn flows through first residual
	dAttnOut := dPostAttn
	dX := dPostAttn // copy via residual

	// Backward through Attention
	dNormed1, _ := tb.Attn.Backward(cache.AttnCache, dAttnOut)

	// Backward through AttnNorm
	dX2, _ := tb.AttnNorm.Backward(cache.X, dNormed1)
	dX = addTensors(dX, dX2)

	return dX, nil
}

// ---- LLM Backward ----

// LLMCache stores all intermediates for full model backward.
type LLMCache struct {
	Tokens      *tensor.Tensor
	Embeddings  *tensor.Tensor // [batch, seqLen, dim]
	BlockCaches []*BlockCache
	Normed      *tensor.Tensor // after final norm
}

// ForwardWithCache runs full LLM and saves intermediates.
func (m *LLM) ForwardWithCache(tokens *tensor.Tensor) (*tensor.Tensor, *LLMCache, error) {
	shape := tokens.Shape()
	batch := shape[0]
	seqLen := shape[1]

	cache := &LLMCache{Tokens: tokens}

	// Embedding
	var embeddings []*tensor.Tensor
	for b := 0; b < batch; b++ {
		bt, _ := sliceBatch(tokens, b, seqLen)
		emb, _ := m.TokEmbed.Forward(bt)
		embeddings = append(embeddings, emb)
	}
	x, _ := stackBatch(embeddings, batch, seqLen, m.Config.Dim)
	cache.Embeddings = x

	// Transformer blocks
	cache.BlockCaches = make([]*BlockCache, len(m.Layers))
	var err error
	for i, layer := range m.Layers {
		var bc *BlockCache
		x, bc, err = layer.ForwardWithCache(x)
		if err != nil {
			return nil, nil, err
		}
		cache.BlockCaches[i] = bc
	}

	// Final norm
	normed, _ := m.Norm.Forward(x)
	cache.Normed = normed

	// Output projection
	logits, _ := m.Output.Forward(normed)

	return logits, cache, nil
}

// Backward runs full model backward from logits gradient.
func (m *LLM) Backward(cache *LLMCache, dLogits *tensor.Tensor) error {
	// Backward through output projection
	dNormed, _ := m.Output.Backward(cache.Normed, dLogits)

	// Backward through final norm
	// Need the input to final norm = output of last transformer block
	lastBlockOut := cache.BlockCaches[len(cache.BlockCaches)-1]
	// Reconstruct: lastBlockOut = PostAttn + FFNOut
	finalNormInput := addTensors(lastBlockOut.PostAttn, lastBlockOut.FFNOut)
	dx, _ := m.Norm.Backward(finalNormInput, dNormed)

	// Backward through transformer blocks (reverse order)
	for i := len(m.Layers) - 1; i >= 0; i-- {
		var err error
		dx, err = m.Layers[i].Backward(cache.BlockCaches[i], dx)
		if err != nil {
			return err
		}
	}

	// Backward through embedding
	for b := 0; b < cache.Tokens.Shape()[0]; b++ {
		seqLen := cache.Tokens.Shape()[1]
		bt, _ := sliceBatch(cache.Tokens, b, seqLen)

		// Extract this batch's embedding gradient
		dxData := dx.ToFloat32Slice()
		dim := m.Config.Dim
		batchGrad := dxData[b*seqLen*dim : (b+1)*seqLen*dim]
		dEmb, _ := tensor.FromSlice(batchGrad, tensor.Shape{seqLen, dim})

		m.TokEmbed.Backward(bt, dEmb)
	}

	return nil
}

// ---- Utility functions ----

// rearrangeBSHD: [batch*seq, heads*headDim] → [batch, heads, seq, headDim] (flat)
func rearrangeBSHD(data []float32, batch, seqLen, numHeads, headDim int) []float32 {
	out := make([]float32, len(data))
	for b := 0; b < batch; b++ {
		for s := 0; s < seqLen; s++ {
			for h := 0; h < numHeads; h++ {
				for d := 0; d < headDim; d++ {
					srcIdx := (b*seqLen+s)*numHeads*headDim + h*headDim + d
					dstIdx := ((b*numHeads+h)*seqLen+s)*headDim + d
					out[dstIdx] = data[srcIdx]
				}
			}
		}
	}
	return out
}

// rearrangeBHSD: [batch, heads, seq, headDim] → [batch, seq, heads*headDim] (flat)
func rearrangeBHSD(data []float32, batch, seqLen, numHeads, headDim int) []float32 {
	out := make([]float32, len(data))
	for b := 0; b < batch; b++ {
		for h := 0; h < numHeads; h++ {
			for s := 0; s < seqLen; s++ {
				for d := 0; d < headDim; d++ {
					srcIdx := ((b*numHeads+h)*seqLen+s)*headDim + d
					dstIdx := (b*seqLen+s)*numHeads*headDim + h*headDim + d
					out[dstIdx] = data[srcIdx]
				}
			}
		}
	}
	return out
}

func applyRoPEInPlace(data []float32, batch, numHeads, seqLen, headDim int, base float64) {
	halfDim := headDim / 2
	for b := 0; b < batch; b++ {
		for h := 0; h < numHeads; h++ {
			for pos := 0; pos < seqLen; pos++ {
				off := ((b*numHeads+h)*seqLen + pos) * headDim
				for i := 0; i < halfDim; i++ {
					freq := 1.0 / math.Pow(base, float64(2*i)/float64(headDim))
					angle := float64(pos) * freq
					cos := float32(math.Cos(angle))
					sin := float32(math.Sin(angle))
					x0 := data[off+i]
					x1 := data[off+halfDim+i]
					data[off+i] = x0*cos - x1*sin
					data[off+halfDim+i] = x0*sin + x1*cos
				}
			}
		}
	}
}

func ropeBackwardInPlace(data []float32, batch, numHeads, seqLen, headDim int, base float64) {
	halfDim := headDim / 2
	for b := 0; b < batch; b++ {
		for h := 0; h < numHeads; h++ {
			for pos := 0; pos < seqLen; pos++ {
				off := ((b*numHeads+h)*seqLen + pos) * headDim
				for i := 0; i < halfDim; i++ {
					freq := 1.0 / math.Pow(base, float64(2*i)/float64(headDim))
					angle := float64(pos) * freq
					cos := float32(math.Cos(angle))
					sin := float32(-math.Sin(angle)) // negative angle for backward
					x0 := data[off+i]
					x1 := data[off+halfDim+i]
					data[off+i] = x0*cos - x1*sin
					data[off+halfDim+i] = x0*sin + x1*cos
				}
			}
		}
	}
}
```

## File: ./nn/backward.go
```go
package nn

import (
	"math"

	"github.com/vugar/goml/tensor"
)

// ---- Linear Backward ----

// Backward computes gradients for Linear layer.
// dout: [batch, seqLen, outF] → dx: [batch, seqLen, inF]
// Also sets gradients on Weight and Bias.
func (l *Linear) Backward(x, dout *tensor.Tensor) (*tensor.Tensor, error) {
	xShape := x.Shape()
	batch := 1
	if len(xShape) == 3 {
		batch = xShape[0] * xShape[1]
	} else if len(xShape) == 2 {
		batch = xShape[0]
	}

	xData := x.ToFloat32Slice()
	doutData := dout.ToFloat32Slice()
	wData := l.Weight.ToFloat32Slice()

	// dx = dout @ W  (not transposed, since W is [outF, inF])
	dxData := make([]float32, batch*l.InF)
	for b := 0; b < batch; b++ {
		for i := 0; i < l.InF; i++ {
			sum := float32(0)
			for o := 0; o < l.OutF; o++ {
				sum += doutData[b*l.OutF+o] * wData[o*l.InF+i]
			}
			dxData[b*l.InF+i] = sum
		}
	}

	// dW = dout^T @ x → [outF, inF]
	dWData := make([]float32, l.OutF*l.InF)
	for b := 0; b < batch; b++ {
		for o := 0; o < l.OutF; o++ {
			for i := 0; i < l.InF; i++ {
				dWData[o*l.InF+i] += doutData[b*l.OutF+o] * xData[b*l.InF+i]
			}
		}
	}

	// Set weight gradient
	dW, err := tensor.FromSlice(dWData, tensor.Shape{l.OutF, l.InF})
	if err != nil {
		return nil, err
	}
	accumulateGrad(l.Weight, dW)

	// dBias = sum(dout, axis=0)
	if l.Bias != nil {
		dBData := make([]float32, l.OutF)
		for b := 0; b < batch; b++ {
			for o := 0; o < l.OutF; o++ {
				dBData[o] += doutData[b*l.OutF+o]
			}
		}
		dB, err := tensor.FromSlice(dBData, tensor.Shape{l.OutF})
		if err != nil {
			return nil, err
		}
		accumulateGrad(l.Bias, dB)
	}

	dx, err := tensor.FromSlice(dxData, xShape)
	if err != nil {
		return nil, err
	}
	return dx, nil
}

// ---- LayerNorm Backward ----

// Backward computes gradients for LayerNorm.
func (ln *LayerNorm) Backward(x, dout *tensor.Tensor) (*tensor.Tensor, error) {
	xData := x.ToFloat32Slice()
	doutData := dout.ToFloat32Slice()
	gammaData := ln.Gamma.ToFloat32Slice()

	shape := x.Shape()
	normSize := shape[len(shape)-1]
	batchSize := x.NumElements() / normSize

	dxData := make([]float32, len(xData))
	dGamma := make([]float32, normSize)
	dBeta := make([]float32, normSize)

	for b := 0; b < batchSize; b++ {
		off := b * normSize

		// Recompute forward stats
		mean := float32(0)
		for i := 0; i < normSize; i++ {
			mean += xData[off+i]
		}
		mean /= float32(normSize)

		variance := float32(0)
		for i := 0; i < normSize; i++ {
			d := xData[off+i] - mean
			variance += d * d
		}
		variance /= float32(normSize)
		invStd := float32(1.0 / math.Sqrt(float64(variance)+ln.Eps))

		// Normalized values
		xNorm := make([]float32, normSize)
		for i := 0; i < normSize; i++ {
			xNorm[i] = (xData[off+i] - mean) * invStd
		}

		// dGamma, dBeta
		for i := 0; i < normSize; i++ {
			dGamma[i] += doutData[off+i] * xNorm[i]
			dBeta[i] += doutData[off+i]
		}

		// dx
		// dx = (1/N) * invStd * (N*dy_hat - sum(dy_hat) - xnorm*sum(dy_hat*xnorm))
		// where dy_hat = dout * gamma
		dyHat := make([]float32, normSize)
		sumDyHat := float32(0)
		sumDyHatXnorm := float32(0)
		for i := 0; i < normSize; i++ {
			dyHat[i] = doutData[off+i] * gammaData[i]
			sumDyHat += dyHat[i]
			sumDyHatXnorm += dyHat[i] * xNorm[i]
		}

		invN := float32(1.0) / float32(normSize)
		for i := 0; i < normSize; i++ {
			dxData[off+i] = invStd * invN * (float32(normSize)*dyHat[i] - sumDyHat - xNorm[i]*sumDyHatXnorm)
		}
	}

	dG, _ := tensor.FromSlice(dGamma, tensor.Shape{normSize})
	dB, _ := tensor.FromSlice(dBeta, tensor.Shape{normSize})
	accumulateGrad(ln.Gamma, dG)
	accumulateGrad(ln.Beta, dB)

	dx, _ := tensor.FromSlice(dxData, shape)
	return dx, nil
}

// ---- FeedForward Backward ----

// Backward computes gradients for FFN.
func (ff *FeedForward) Backward(x, dout *tensor.Tensor) (*tensor.Tensor, error) {
	if ff.UseSwiGLU {
		return ff.backwardSwiGLU(x, dout)
	}
	return ff.backwardStandard(x, dout)
}

func (ff *FeedForward) backwardSwiGLU(x, dout *tensor.Tensor) (*tensor.Tensor, error) {
	// Forward: gate = SiLU(W1(x)), up = W3(x), hidden = gate * up, out = W2(hidden)

	// Recompute forward
	w1out, _ := ff.W1.Forward(x)
	gate := siluForward(w1out)
	up, _ := ff.W3.Forward(x)
	hidden := mulElementwise(gate, up)

	// Backward through W2
	dHidden, err := ff.W2.Backward(hidden, dout)
	if err != nil {
		return nil, err
	}

	// Backward through gate * up
	dHiddenData := dHidden.ToFloat32Slice()
	gateData := gate.ToFloat32Slice()
	upData := up.ToFloat32Slice()

	dGateData := make([]float32, len(gateData))
	dUpData := make([]float32, len(upData))
	for i := range dHiddenData {
		dGateData[i] = dHiddenData[i] * upData[i]
		dUpData[i] = dHiddenData[i] * gateData[i]
	}

	dGate, _ := tensor.FromSlice(dGateData, gate.Shape())
	dUp, _ := tensor.FromSlice(dUpData, up.Shape())

	// Backward through SiLU
	dSilu := siluBackward(w1out, dGate)

	// Backward through W1 and W3
	dx1, err := ff.W1.Backward(x, dSilu)
	if err != nil {
		return nil, err
	}
	dx3, err := ff.W3.Backward(x, dUp)
	if err != nil {
		return nil, err
	}

	// dx = dx1 + dx3
	return addTensors(dx1, dx3), nil
}

func (ff *FeedForward) backwardStandard(x, dout *tensor.Tensor) (*tensor.Tensor, error) {
	// Forward: h = GELU(W1(x)), out = W2(h)
	w1out, _ := ff.W1.Forward(x)
	h := geluForward(w1out)

	// Backward through W2
	dH, err := ff.W2.Backward(h, dout)
	if err != nil {
		return nil, err
	}

	// Backward through GELU
	dGelu := geluBackward(w1out, dH)

	// Backward through W1
	return ff.W1.Backward(x, dGelu)
}

// ---- Embedding Backward ----

// Backward computes gradients for Embedding.
// dout: [seqLen, embedDim] → scatters gradients back to Weight
func (e *Embedding) Backward(indices *tensor.Tensor, dout *tensor.Tensor) error {
	iData := indices.ToInt64Slice()
	doutData := dout.ToFloat32Slice()
	seqLen := len(iData)

	dWData := make([]float32, e.VocabSize*e.EmbedDim)
	for s := 0; s < seqLen; s++ {
		idx := int(iData[s])
		for d := 0; d < e.EmbedDim; d++ {
			dWData[idx*e.EmbedDim+d] += doutData[s*e.EmbedDim+d]
		}
	}

	dW, _ := tensor.FromSlice(dWData, tensor.Shape{e.VocabSize, e.EmbedDim})
	accumulateGrad(e.Weight, dW)
	return nil
}

// ---- Helper functions ----

func accumulateGrad(param, grad *tensor.Tensor) {
	if param.Grad() == nil {
		param.SetGrad(grad)
	} else {
		pGrad := param.Grad().ToFloat32Slice()
		gData := grad.ToFloat32Slice()
		for i := range pGrad {
			pGrad[i] += gData[i]
		}
	}
}

func siluForward(x *tensor.Tensor) *tensor.Tensor {
	data := x.ToFloat32Slice()
	out := make([]float32, len(data))
	for i, v := range data {
		sig := float32(1.0 / (1.0 + math.Exp(float64(-v))))
		out[i] = v * sig
	}
	t, _ := tensor.FromSlice(out, x.Shape())
	return t
}

func siluBackward(x, dout *tensor.Tensor) *tensor.Tensor {
	xData := x.ToFloat32Slice()
	dData := dout.ToFloat32Slice()
	out := make([]float32, len(xData))
	for i, v := range xData {
		sig := float32(1.0 / (1.0 + math.Exp(float64(-v))))
		// d(x*sigmoid(x))/dx = sigmoid(x) + x*sigmoid(x)*(1-sigmoid(x))
		//                    = sigmoid(x) * (1 + x*(1-sigmoid(x)))
		out[i] = dData[i] * sig * (1 + v*(1-sig))
	}
	t, _ := tensor.FromSlice(out, x.Shape())
	return t
}

func geluForward(x *tensor.Tensor) *tensor.Tensor {
	data := x.ToFloat32Slice()
	c := float32(math.Sqrt(2.0 / math.Pi))
	out := make([]float32, len(data))
	for i, v := range data {
		out[i] = 0.5 * v * (1 + float32(math.Tanh(float64(c*(v+0.044715*v*v*v)))))
	}
	t, _ := tensor.FromSlice(out, x.Shape())
	return t
}

func geluBackward(x, dout *tensor.Tensor) *tensor.Tensor {
	xData := x.ToFloat32Slice()
	dData := dout.ToFloat32Slice()
	c := float64(math.Sqrt(2.0 / math.Pi))
	out := make([]float32, len(xData))
	for i, v := range xData {
		vf := float64(v)
		inner := c * (vf + 0.044715*vf*vf*vf)
		tanh := math.Tanh(inner)
		dtanh := 1 - tanh*tanh
		dinner := c * (1 + 3*0.044715*vf*vf)
		grad := 0.5*(1+tanh) + 0.5*vf*dtanh*dinner
		out[i] = dData[i] * float32(grad)
	}
	t, _ := tensor.FromSlice(out, x.Shape())
	return t
}

func mulElementwise(a, b *tensor.Tensor) *tensor.Tensor {
	aData := a.ToFloat32Slice()
	bData := b.ToFloat32Slice()
	out := make([]float32, len(aData))
	for i := range out {
		out[i] = aData[i] * bData[i]
	}
	t, _ := tensor.FromSlice(out, a.Shape())
	return t
}

func addTensors(a, b *tensor.Tensor) *tensor.Tensor {
	aData := a.ToFloat32Slice()
	bData := b.ToFloat32Slice()
	out := make([]float32, len(aData))
	for i := range out {
		out[i] = aData[i] + bData[i]
	}
	t, _ := tensor.FromSlice(out, a.Shape())
	return t
}
```

## File: ./nn/embedding.go
```go
package nn

import (
	"math/rand"

	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/tensor"
)

// Embedding is a lookup table for token embeddings.
type Embedding struct {
	Weight    *tensor.Tensor // [vocabSize, embedDim]
	VocabSize int
	EmbedDim  int
}

// NewEmbedding creates an embedding layer with normal initialization.
func NewEmbedding(vocabSize, embedDim int, device backend.Device) (*Embedding, error) {
	data := make([]float32, vocabSize*embedDim)
	for i := range data {
		data[i] = float32(rand.NormFloat64() * 0.02)
	}

	w, err := tensor.FromSlice(data, tensor.Shape{vocabSize, embedDim})
	if err != nil {
		return nil, err
	}
	w.SetRequiresGrad(true)

	return &Embedding{Weight: w, VocabSize: vocabSize, EmbedDim: embedDim}, nil
}

// Forward looks up embeddings for given token indices.
// indices shape: [seqLen] (int64) → output: [seqLen, embedDim]
func (e *Embedding) Forward(indices *tensor.Tensor) (*tensor.Tensor, error) {
	seqLen := indices.NumElements()

	bk, err := backend.GetForDevice(indices.Device())
	if err != nil {
		return nil, err
	}

	outStore, err := bk.Alloc(seqLen * e.EmbedDim * int(tensor.Float32.Size()))
	if err != nil {
		return nil, err
	}

	err = bk.Embedding(outStore, e.Weight.Storage(), indices.Storage(),
		e.VocabSize, e.EmbedDim, seqLen, tensor.Float32)
	if err != nil {
		return nil, err
	}

	return tensor.NewTensor(outStore, tensor.Shape{seqLen, e.EmbedDim}, tensor.Float32), nil
}

// Parameters returns trainable parameters.
func (e *Embedding) Parameters() []*tensor.Tensor {
	return []*tensor.Tensor{e.Weight}
}
```

## File: ./nn/feedforward.go
```go
package nn

import (
	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/ops"
	"github.com/vugar/goml/tensor"
)

// FeedForward implements the FFN block.
// SwiGLU variant (LLaMA style): out = W2(SiLU(W1(x)) * W3(x))
// Standard variant: out = W2(GELU(W1(x)))
type FeedForward struct {
	W1     *Linear // gate projection   [dim, hiddenDim]
	W2     *Linear // down projection   [hiddenDim, dim]
	W3     *Linear // up projection     [dim, hiddenDim] (SwiGLU only)
	UseSwiGLU bool
}

// NewFeedForward creates a feed-forward block.
// hiddenDim is typically 4*dim for standard, or (2/3)*4*dim for SwiGLU.
func NewFeedForward(dim, hiddenDim int, useSwiGLU bool, device backend.Device) (*FeedForward, error) {
	w1, err := NewLinear(dim, hiddenDim, false, device)
	if err != nil {
		return nil, err
	}
	w2, err := NewLinear(hiddenDim, dim, false, device)
	if err != nil {
		return nil, err
	}

	ff := &FeedForward{W1: w1, W2: w2, UseSwiGLU: useSwiGLU}

	if useSwiGLU {
		w3, err := NewLinear(dim, hiddenDim, false, device)
		if err != nil {
			return nil, err
		}
		ff.W3 = w3
	}

	return ff, nil
}

// Forward runs the FFN.
// x: [batch, seqLen, dim] → [batch, seqLen, dim]
func (ff *FeedForward) Forward(x *tensor.Tensor) (*tensor.Tensor, error) {
	if ff.UseSwiGLU {
		return ff.forwardSwiGLU(x)
	}
	return ff.forwardStandard(x)
}

// forwardSwiGLU: out = W2(SiLU(W1(x)) * W3(x))
func (ff *FeedForward) forwardSwiGLU(x *tensor.Tensor) (*tensor.Tensor, error) {
	// Gate: SiLU(W1(x))
	gate, err := ff.W1.Forward(x)
	if err != nil {
		return nil, err
	}
	gate, err = ops.Silu(gate)
	if err != nil {
		return nil, err
	}

	// Up: W3(x)
	up, err := ff.W3.Forward(x)
	if err != nil {
		return nil, err
	}

	// Element-wise multiply: gate * up
	hidden, err := ops.Mul(gate, up)
	if err != nil {
		return nil, err
	}

	// Down: W2(hidden)
	return ff.W2.Forward(hidden)
}

// forwardStandard: out = W2(GELU(W1(x)))
func (ff *FeedForward) forwardStandard(x *tensor.Tensor) (*tensor.Tensor, error) {
	h, err := ff.W1.Forward(x)
	if err != nil {
		return nil, err
	}
	h, err = ops.Gelu(h)
	if err != nil {
		return nil, err
	}
	return ff.W2.Forward(h)
}

// Parameters returns all trainable parameters.
func (ff *FeedForward) Parameters() []*tensor.Tensor {
	var params []*tensor.Tensor
	params = append(params, ff.W1.Parameters()...)
	params = append(params, ff.W2.Parameters()...)
	if ff.W3 != nil {
		params = append(params, ff.W3.Parameters()...)
	}
	return params
}
```

## File: ./nn/layernorm.go
```go
package nn

import (
	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/ops"
	"github.com/vugar/goml/tensor"
)

// LayerNorm implements layer normalization.
type LayerNorm struct {
	Gamma *tensor.Tensor // [normSize] scale
	Beta  *tensor.Tensor // [normSize] shift
	Eps   float64
	Axis  int
}

// NewLayerNorm creates a layer norm with gamma=1, beta=0.
func NewLayerNorm(normSize int, eps float64, device backend.Device) (*LayerNorm, error) {
	gamma, err := tensor.Ones(tensor.Shape{normSize}, tensor.Float32, device)
	if err != nil {
		return nil, err
	}
	gamma.SetRequiresGrad(true)

	beta, err := tensor.Zeros(tensor.Shape{normSize}, tensor.Float32, device)
	if err != nil {
		return nil, err
	}
	beta.SetRequiresGrad(true)

	return &LayerNorm{Gamma: gamma, Beta: beta, Eps: eps, Axis: -1}, nil
}

// Forward applies layer normalization.
// x shape: [..., normSize] → same shape
func (ln *LayerNorm) Forward(x *tensor.Tensor) (*tensor.Tensor, error) {
	axis := ln.Axis
	if axis < 0 {
		axis = x.NDim() + axis // -1 → last axis
	}
	return ops.LayerNorm(x, ln.Gamma, ln.Beta, axis, ln.Eps)
}

// Parameters returns trainable parameters.
func (ln *LayerNorm) Parameters() []*tensor.Tensor {
	return []*tensor.Tensor{ln.Gamma, ln.Beta}
}
```

## File: ./nn/linear.go
```go
package nn

import (
	"math"
	"math/rand"

	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/ops"
	"github.com/vugar/goml/tensor"
)

// Linear implements y = x @ W^T + bias
type Linear struct {
	Weight *tensor.Tensor // [outFeatures, inFeatures]
	Bias   *tensor.Tensor // [outFeatures] or nil
	InF    int
	OutF   int
}

// NewLinear creates a linear layer with Kaiming initialization.
func NewLinear(inFeatures, outFeatures int, bias bool, device backend.Device) (*Linear, error) {
	// Kaiming He init: scale = sqrt(2 / fan_in)
	scale := math.Sqrt(2.0 / float64(inFeatures))

	wData := make([]float32, outFeatures*inFeatures)
	for i := range wData {
		wData[i] = float32(rand.NormFloat64() * scale)
	}

	w, err := tensor.FromSlice(wData, tensor.Shape{outFeatures, inFeatures})
	if err != nil {
		return nil, err
	}
	w.SetRequiresGrad(true)

	l := &Linear{Weight: w, InF: inFeatures, OutF: outFeatures}

	if bias {
		bData := make([]float32, outFeatures)
		b, err := tensor.FromSlice(bData, tensor.Shape{outFeatures})
		if err != nil {
			return nil, err
		}
		b.SetRequiresGrad(true)
		l.Bias = b
	}

	return l, nil
}

// Forward computes y = x @ W^T + bias.
// x shape: [..., inFeatures] → output: [..., outFeatures]
func (l *Linear) Forward(x *tensor.Tensor) (*tensor.Tensor, error) {
	// W^T: [inFeatures, outFeatures]
	wT, err := l.Weight.T()
	if err != nil {
		return nil, err
	}

	// x @ W^T: [..., inFeatures] @ [inFeatures, outFeatures] = [..., outFeatures]
	out, err := ops.MatMul(x, wT)
	if err != nil {
		return nil, err
	}

	if l.Bias != nil {
		out, err = ops.Add(out, l.Bias)
		if err != nil {
			return nil, err
		}
	}

	return out, nil
}

// Parameters returns all trainable parameters.
func (l *Linear) Parameters() []*tensor.Tensor {
	if l.Bias != nil {
		return []*tensor.Tensor{l.Weight, l.Bias}
	}
	return []*tensor.Tensor{l.Weight}
}
```

## File: ./nn/loss.go
```go
package nn

import (
	"math"

	"github.com/vugar/goml/tensor"
)

// CrossEntropyLoss computes cross-entropy loss for language modeling.
// logits: [batch, seqLen, vocabSize] — raw model outputs
// targets: [batch, seqLen] — target token indices (int64)
// Returns: scalar loss tensor + populates gradients on logits.
//
// Loss = -1/N * Σ log(softmax(logits)[target])
// Grad = softmax(logits) - one_hot(target)  (divided by N)
func CrossEntropyLoss(logits *tensor.Tensor, targets *tensor.Tensor) (*tensor.Tensor, error) {
	logitsShape := logits.Shape()
	batch := logitsShape[0]
	seqLen := logitsShape[1]
	vocabSize := logitsShape[2]
	N := batch * seqLen // total tokens

	logitsData := logits.ToFloat32Slice()
	targetsData := targets.ToInt64Slice()

	totalLoss := float64(0)

	// Gradient buffer (same shape as logits)
	gradData := make([]float32, len(logitsData))

	for b := 0; b < batch; b++ {
		for s := 0; s < seqLen; s++ {
			offset := (b*seqLen + s) * vocabSize
			target := int(targetsData[b*seqLen+s])

			// Numerically stable softmax + cross-entropy
			// Step 1: find max
			maxVal := float32(-math.MaxFloat32)
			for v := 0; v < vocabSize; v++ {
				if logitsData[offset+v] > maxVal {
					maxVal = logitsData[offset+v]
				}
			}

			// Step 2: exp(x - max) and sum
			sumExp := float32(0)
			for v := 0; v < vocabSize; v++ {
				gradData[offset+v] = float32(math.Exp(float64(logitsData[offset+v] - maxVal)))
				sumExp += gradData[offset+v]
			}

			// Step 3: normalize to get probabilities (stored in gradData)
			for v := 0; v < vocabSize; v++ {
				gradData[offset+v] /= sumExp
			}

			// Step 4: loss = -log(prob[target])
			prob := gradData[offset+target]
			if prob < 1e-10 {
				prob = 1e-10
			}
			totalLoss -= math.Log(float64(prob))

			// Step 5: gradient = prob - 1(target) / N
			gradData[offset+target] -= 1.0
			for v := 0; v < vocabSize; v++ {
				gradData[offset+v] /= float32(N)
			}
		}
	}

	avgLoss := float32(totalLoss / float64(N))

	// Create loss scalar tensor
	lossTensor, err := tensor.FromSlice([]float32{avgLoss}, tensor.Shape{1})
	if err != nil {
		return nil, err
	}

	// Create gradient tensor and attach to logits
	gradTensor, err := tensor.FromSlice(gradData, logitsShape)
	if err != nil {
		return nil, err
	}
	logits.SetGrad(gradTensor)

	return lossTensor, nil
}

// ManualBackward propagates gradients through the model manually.
// This is simpler than full autograd for the standard LLM training case:
// loss → output_linear → transformer_blocks → embedding
//
// For now we do gradient propagation for the output linear layer
// and then stop (the key insight: we need working gradients on all parameters).
func ManualBackward(model *LLM, logits *tensor.Tensor) {
	// The gradient on logits is already set by CrossEntropyLoss.
	// We need to propagate it through the output linear layer
	// and then through each transformer block.
	//
	// For the output linear: y = x @ W^T + b
	// dL/dW = (dL/dy)^T @ x
	// dL/dx = dL/dy @ W
	// dL/db = sum(dL/dy, axis=0)
	//
	// This is a simplified backward that works for our architecture.

	gradLogits := logits.Grad()
	if gradLogits == nil {
		return
	}

	// We'll do a simple gradient computation for each layer
	// by numerical approximation for now, upgrading to analytic later.
	// For the MVP, we propagate through the output layer analytically.

	propagateLinearBackward(model.Output, gradLogits, nil)
}

// propagateLinearBackward computes gradients for a linear layer.
// gradOutput: [batch, seqLen, outFeatures]
// Returns gradient w.r.t. input: [batch, seqLen, inFeatures]
func propagateLinearBackward(l *Linear, gradOutput *tensor.Tensor, input *tensor.Tensor) {
	goShape := gradOutput.Shape()
	batch := goShape[0]
	seqLen := goShape[1]
	outF := l.OutF
	inF := l.InF

	goData := gradOutput.ToFloat32Slice()
	wData := l.Weight.ToFloat32Slice()

	// dL/dW = gradOutput^T @ input (summed over batch*seq)
	// For now, just compute weight gradients from gradOutput
	wGrad := make([]float32, outF*inF)

	if input != nil {
		inData := input.ToFloat32Slice()
		for b := 0; b < batch; b++ {
			for s := 0; s < seqLen; s++ {
				goOff := (b*seqLen + s) * outF
				inOff := (b*seqLen + s) * inF
				for o := 0; o < outF; o++ {
					for i := 0; i < inF; i++ {
						wGrad[o*inF+i] += goData[goOff+o] * inData[inOff+i]
					}
				}
			}
		}
	}

	wGradTensor, _ := tensor.FromSlice(wGrad, tensor.Shape{outF, inF})
	l.Weight.SetGrad(wGradTensor)

	if l.Bias != nil {
		bGrad := make([]float32, outF)
		for b := 0; b < batch; b++ {
			for s := 0; s < seqLen; s++ {
				goOff := (b*seqLen + s) * outF
				for o := 0; o < outF; o++ {
					bGrad[o] += goData[goOff+o]
				}
			}
		}
		bGradTensor, _ := tensor.FromSlice(bGrad, tensor.Shape{outF})
		l.Bias.SetGrad(bGradTensor)
	}

	_ = wData // used for input gradient computation (not needed for now)
}

// SimpleBackward does a full backward pass using finite differences
// for parameter gradient estimation. Slow but correct — good for validation.
// After validation, replace with analytic gradients.
//
// Actually, let's do proper analytic backward through the whole model.
// This requires caching forward pass intermediates.

// ForwardWithCache runs forward pass and caches all intermediates for backward.
type ForwardCache struct {
	Embeddings   *tensor.Tensor   // [batch, seqLen, dim]
	LayerInputs  []*tensor.Tensor // input to each transformer block
	LayerOutputs []*tensor.Tensor // output of each transformer block
	NormOutput   *tensor.Tensor   // after final norm
	Logits       *tensor.Tensor   // final logits
}

// ForwardCached runs forward pass with caching for backward.
func (m *LLM) ForwardCached(tokens *tensor.Tensor) (*tensor.Tensor, *ForwardCache, error) {
	cache := &ForwardCache{}
	shape := tokens.Shape()
	batch := shape[0]
	seqLen := shape[1]

	// Embedding
	var embeddings []*tensor.Tensor
	for b := 0; b < batch; b++ {
		batchTokens, err := sliceBatch(tokens, b, seqLen)
		if err != nil {
			return nil, nil, err
		}
		emb, err := m.TokEmbed.Forward(batchTokens)
		if err != nil {
			return nil, nil, err
		}
		embeddings = append(embeddings, emb)
	}

	x, err := stackBatch(embeddings, batch, seqLen, m.Config.Dim)
	if err != nil {
		return nil, nil, err
	}
	cache.Embeddings = x

	// Transformer layers
	cache.LayerInputs = make([]*tensor.Tensor, len(m.Layers))
	cache.LayerOutputs = make([]*tensor.Tensor, len(m.Layers))

	for i, layer := range m.Layers {
		cache.LayerInputs[i] = x
		x, err = layer.Forward(x)
		if err != nil {
			return nil, nil, err
		}
		cache.LayerOutputs[i] = x
	}

	// Final norm
	x, err = m.Norm.Forward(x)
	if err != nil {
		return nil, nil, err
	}
	cache.NormOutput = x

	// Output projection
	logits, err := m.Output.Forward(x)
	if err != nil {
		return nil, nil, err
	}
	cache.Logits = logits

	return logits, cache, nil
}

// BackwardFromLoss propagates gradients from cross-entropy loss through the model.
// Uses cached intermediates for efficient computation.
func (m *LLM) BackwardFromLoss(cache *ForwardCache) {
	// Gradient on logits is already set by CrossEntropyLoss.
	// Propagate through output linear using cached norm output as input.
	propagateLinearBackward(m.Output, cache.Logits.Grad(), cache.NormOutput)

	// For transformer layers, we use a simplified gradient:
	// Each parameter gets a gradient proportional to the loss gradient
	// flowing through it. Full analytic backward through attention + FFN
	// is complex, so we use the gradients from the output layer
	// and apply a scaled version to all parameters.
	//
	// This is a HACK for the MVP. Proper backward requires:
	// 1. Cache all intermediate activations in attention (Q, K, V, scores, etc.)
	// 2. Implement backward for softmax, matmul, layernorm, silu, etc.
	// 3. Chain them all together
	//
	// For now, the output layer gradients are correct, and we apply
	// gradient noise to other parameters to enable learning (like REINFORCE).

	// Actually, let's do something smarter: use the output layer's correct
	// gradient to train the output layer properly, and for other layers
	// use a simple signal: the gradient from the loss.
	//
	// Even with just training the output head + embedding, the model
	// can learn meaningful token distributions.
}
```

## File: ./nn/model.go
```go
package nn

import (
	"fmt"

	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/tensor"
)

// ModelConfig defines the architecture hyperparameters.
type ModelConfig struct {
	VocabSize    int     // vocabulary size
	Dim          int     // model dimension (embedding size)
	NumLayers    int     // number of transformer blocks
	NumHeads     int     // number of attention heads
	FFNHiddenDim int    // FFN intermediate size
	UseSwiGLU    bool   // use SwiGLU activation in FFN
	MaxSeqLen    int     // maximum sequence length
	NormEps      float64 // layer norm epsilon
}

// SmallConfig returns config for a ~25M parameter model (for testing).
func SmallConfig() ModelConfig {
	return ModelConfig{
		VocabSize:    32000,
		Dim:          256,
		NumLayers:    6,
		NumHeads:     8,
		FFNHiddenDim: 688, // (2/3)*4*256 ≈ 688 for SwiGLU
		UseSwiGLU:    true,
		MaxSeqLen:    512,
		NormEps:      1e-5,
	}
}

// TinyConfig returns config for a ~3M parameter model (for quick testing).
func TinyConfig() ModelConfig {
	return ModelConfig{
		VocabSize:    256,  // byte-level
		Dim:          64,
		NumLayers:    2,
		NumHeads:     4,
		FFNHiddenDim: 172, // (2/3)*4*64
		UseSwiGLU:    true,
		MaxSeqLen:    128,
		NormEps:      1e-5,
	}
}

// LLM is the complete language model.
type LLM struct {
	Config    ModelConfig
	TokEmbed *Embedding          // token embedding
	Layers   []*TransformerBlock  // transformer layers
	Norm     *LayerNorm           // final layer norm
	Output   *Linear              // lm_head: dim → vocab
}

// NewLLM creates a language model from config.
func NewLLM(cfg ModelConfig, device backend.Device) (*LLM, error) {
	tokEmbed, err := NewEmbedding(cfg.VocabSize, cfg.Dim, device)
	if err != nil {
		return nil, fmt.Errorf("embedding: %w", err)
	}

	layers := make([]*TransformerBlock, cfg.NumLayers)
	for i := 0; i < cfg.NumLayers; i++ {
		layer, err := NewTransformerBlock(cfg.Dim, cfg.NumHeads, cfg.FFNHiddenDim, cfg.UseSwiGLU, device)
		if err != nil {
			return nil, fmt.Errorf("layer %d: %w", i, err)
		}
		layers[i] = layer
	}

	norm, err := NewLayerNorm(cfg.Dim, cfg.NormEps, device)
	if err != nil {
		return nil, fmt.Errorf("final norm: %w", err)
	}

	output, err := NewLinear(cfg.Dim, cfg.VocabSize, false, device)
	if err != nil {
		return nil, fmt.Errorf("output: %w", err)
	}

	return &LLM{
		Config:    cfg,
		TokEmbed: tokEmbed,
		Layers:   layers,
		Norm:     norm,
		Output:   output,
	}, nil
}

// Forward runs the full model.
// tokens: [batch, seqLen] (int64) → logits: [batch, seqLen, vocabSize]
func (m *LLM) Forward(tokens *tensor.Tensor) (*tensor.Tensor, error) {
	shape := tokens.Shape()
	batch := shape[0]
	seqLen := shape[1]

	// Token embedding: [batch, seqLen] → [batch, seqLen, dim]
	// Process each batch element separately then combine
	var embeddings []*tensor.Tensor
	for b := 0; b < batch; b++ {
		// Get this batch's tokens
		batchTokens, err := sliceBatch(tokens, b, seqLen)
		if err != nil {
			return nil, fmt.Errorf("batch slice: %w", err)
		}

		emb, err := m.TokEmbed.Forward(batchTokens)
		if err != nil {
			return nil, fmt.Errorf("embedding: %w", err)
		}
		embeddings = append(embeddings, emb)
	}

	// Stack into [batch, seqLen, dim]
	x, err := stackBatch(embeddings, batch, seqLen, m.Config.Dim)
	if err != nil {
		return nil, fmt.Errorf("stack: %w", err)
	}

	// Pass through transformer layers
	for i, layer := range m.Layers {
		x, err = layer.Forward(x)
		if err != nil {
			return nil, fmt.Errorf("layer %d: %w", i, err)
		}
	}

	// Final layer norm
	x, err = m.Norm.Forward(x)
	if err != nil {
		return nil, fmt.Errorf("final norm: %w", err)
	}

	// Project to vocabulary: [batch, seqLen, dim] → [batch, seqLen, vocabSize]
	logits, err := m.Output.Forward(x)
	if err != nil {
		return nil, fmt.Errorf("output: %w", err)
	}

	return logits, nil
}

// CountParameters returns total number of trainable parameters.
func (m *LLM) CountParameters() int {
	total := 0
	for _, p := range m.Parameters() {
		total += p.NumElements()
	}
	return total
}

// Parameters returns all trainable parameters.
func (m *LLM) Parameters() []*tensor.Tensor {
	var params []*tensor.Tensor
	params = append(params, m.TokEmbed.Parameters()...)
	for _, layer := range m.Layers {
		params = append(params, layer.Parameters()...)
	}
	params = append(params, m.Norm.Parameters()...)
	params = append(params, m.Output.Parameters()...)
	return params
}

// sliceBatch extracts one batch element's tokens.
func sliceBatch(tokens *tensor.Tensor, batchIdx, seqLen int) (*tensor.Tensor, error) {
	allData := tokens.ToInt64Slice()
	start := batchIdx * seqLen
	batchData := make([]int64, seqLen)
	copy(batchData, allData[start:start+seqLen])
	return tensor.FromSlice(batchData, tensor.Shape{seqLen})
}

// stackBatch combines batch embeddings into [batch, seqLen, dim].
func stackBatch(embeddings []*tensor.Tensor, batch, seqLen, dim int) (*tensor.Tensor, error) {
	totalSize := batch * seqLen * dim
	data := make([]float32, totalSize)

	for b := 0; b < batch; b++ {
		embData := embeddings[b].ToFloat32Slice()
		copy(data[b*seqLen*dim:(b+1)*seqLen*dim], embData)
	}

	return tensor.FromSlice(data, tensor.Shape{batch, seqLen, dim})
}

// Softmax sampling utilities

// ArgMax returns the index of the maximum value in a 1D tensor.
func ArgMax(t *tensor.Tensor) int {
	data := t.ToFloat32Slice()
	maxIdx := 0
	maxVal := data[0]
	for i, v := range data {
		if v > maxVal {
			maxVal = v
			maxIdx = i
		}
	}
	return maxIdx
}

// TopKSample samples from top-k logits with temperature.
func TopKSample(logits *tensor.Tensor, k int, temperature float32) int {
	data := logits.ToFloat32Slice()
	n := len(data)

	// Apply temperature
	if temperature != 1.0 {
		for i := range data {
			data[i] /= temperature
		}
	}

	// Find top-k indices
	type indexVal struct {
		idx int
		val float32
	}
	items := make([]indexVal, n)
	for i, v := range data {
		items[i] = indexVal{i, v}
	}

	// Partial sort: get top k
	for i := 0; i < k && i < n; i++ {
		maxJ := i
		for j := i + 1; j < n; j++ {
			if items[j].val > items[maxJ].val {
				maxJ = j
			}
		}
		items[i], items[maxJ] = items[maxJ], items[i]
	}

	if k > n {
		k = n
	}

	// Softmax over top-k
	maxVal := items[0].val
	sumExp := float32(0)
	probs := make([]float32, k)
	for i := 0; i < k; i++ {
		probs[i] = float32Exp(items[i].val - maxVal)
		sumExp += probs[i]
	}
	for i := range probs {
		probs[i] /= sumExp
	}

	// Sample from distribution
	r := float32Rand()
	cumSum := float32(0)
	for i := 0; i < k; i++ {
		cumSum += probs[i]
		if r < cumSum {
			return items[i].idx
		}
	}
	return items[k-1].idx
}

func float32Exp(x float32) float32 {
	if x > 88 {
		return 3.4e38
	}
	if x < -88 {
		return 0
	}
	// Fast approximation
	return float32(1) + x + x*x/2 + x*x*x/6
}

func float32Rand() float32 {
	// Simple LCG for sampling (not cryptographic)
	return float32(pseudoRand()) / float32(1<<31)
}

var prngState uint32 = 42

func pseudoRand() uint32 {
	prngState = prngState*1103515245 + 12345
	return (prngState >> 16) & 0x7FFF
}
```

## File: ./nn/optimizer.go
```go
package nn

import (
	"math"

	"github.com/vugar/goml/tensor"
)

// AdamW implements the AdamW optimizer (decoupled weight decay).
// AdamW: θ = θ - lr * (m_hat / (sqrt(v_hat) + eps) + wd * θ)
type AdamW struct {
	Params      []*tensor.Tensor
	LR          float64
	Beta1       float64
	Beta2       float64
	Eps         float64
	WeightDecay float64
	Step        int

	// State: first and second moment estimates per parameter
	M [][]float32 // first moment (mean of gradients)
	V [][]float32 // second moment (mean of squared gradients)
}

// NewAdamW creates an AdamW optimizer.
func NewAdamW(params []*tensor.Tensor, lr, beta1, beta2, eps, weightDecay float64) *AdamW {
	m := make([][]float32, len(params))
	v := make([][]float32, len(params))
	for i, p := range params {
		n := p.NumElements()
		m[i] = make([]float32, n)
		v[i] = make([]float32, n)
	}

	return &AdamW{
		Params:      params,
		LR:          lr,
		Beta1:       beta1,
		Beta2:       beta2,
		Eps:         eps,
		WeightDecay: weightDecay,
		M:           m,
		V:           v,
	}
}

// DefaultAdamW creates AdamW with standard hyperparameters.
func DefaultAdamW(params []*tensor.Tensor, lr float64) *AdamW {
	return NewAdamW(params, lr, 0.9, 0.999, 1e-8, 0.01)
}

// ZeroGrad clears all parameter gradients.
func (opt *AdamW) ZeroGrad() {
	for _, p := range opt.Params {
		if g := p.Grad(); g != nil {
			gData := g.ToFloat32Slice()
			for i := range gData {
				gData[i] = 0
			}
		}
	}
}

// StepUpdate performs one optimization step.
func (opt *AdamW) StepUpdate() {
	opt.Step++
	t := float64(opt.Step)

	// Bias correction factors
	bc1 := 1.0 - math.Pow(opt.Beta1, t)
	bc2 := 1.0 - math.Pow(opt.Beta2, t)

	lr := opt.LR

	for i, p := range opt.Params {
		grad := p.Grad()
		if grad == nil {
			continue
		}

		pData := p.ToFloat32Slice()
		gData := grad.ToFloat32Slice()
		mData := opt.M[i]
		vData := opt.V[i]

		for j := 0; j < len(pData); j++ {
			g := gData[j]

			// Update biased first moment: m = β1*m + (1-β1)*g
			mData[j] = float32(opt.Beta1)*mData[j] + float32(1-opt.Beta1)*g

			// Update biased second moment: v = β2*v + (1-β2)*g²
			vData[j] = float32(opt.Beta2)*vData[j] + float32(1-opt.Beta2)*g*g

			// Bias-corrected estimates
			mHat := float64(mData[j]) / bc1
			vHat := float64(vData[j]) / bc2

			// AdamW update: param -= lr * (m_hat / (sqrt(v_hat) + eps) + wd * param)
			update := mHat / (math.Sqrt(vHat) + opt.Eps)
			pData[j] -= float32(lr) * (float32(update) + float32(opt.WeightDecay)*pData[j])
		}
	}
}

// SetLR updates the learning rate (for scheduling).
func (opt *AdamW) SetLR(lr float64) {
	opt.LR = lr
}

// CosineSchedule returns the learning rate for a cosine annealing schedule.
// lr = min_lr + 0.5 * (max_lr - min_lr) * (1 + cos(pi * step / total_steps))
func CosineSchedule(step, warmupSteps, totalSteps int, maxLR, minLR float64) float64 {
	if step < warmupSteps {
		// Linear warmup
		return maxLR * float64(step) / float64(warmupSteps)
	}
	// Cosine decay
	progress := float64(step-warmupSteps) / float64(totalSteps-warmupSteps)
	if progress > 1 {
		progress = 1
	}
	return minLR + 0.5*(maxLR-minLR)*(1+math.Cos(math.Pi*progress))
}
```

## File: ./nn/transformer.go
```go
package nn

import (
	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/ops"
	"github.com/vugar/goml/tensor"
)

// TransformerBlock is one layer of the transformer.
// Pre-norm architecture (LLaMA style):
//   x = x + Attention(LayerNorm(x))
//   x = x + FFN(LayerNorm(x))
type TransformerBlock struct {
	AttnNorm *LayerNorm
	Attn     *MultiHeadAttention
	FFNNorm  *LayerNorm
	FFN      *FeedForward
}

// NewTransformerBlock creates one transformer layer.
func NewTransformerBlock(dim, numHeads, ffnHiddenDim int, useSwiGLU bool, device backend.Device) (*TransformerBlock, error) {
	attnNorm, err := NewLayerNorm(dim, 1e-5, device)
	if err != nil {
		return nil, err
	}

	attn, err := NewMultiHeadAttention(dim, numHeads, device)
	if err != nil {
		return nil, err
	}

	ffnNorm, err := NewLayerNorm(dim, 1e-5, device)
	if err != nil {
		return nil, err
	}

	ffn, err := NewFeedForward(dim, ffnHiddenDim, useSwiGLU, device)
	if err != nil {
		return nil, err
	}

	return &TransformerBlock{
		AttnNorm: attnNorm,
		Attn:     attn,
		FFNNorm:  ffnNorm,
		FFN:      ffn,
	}, nil
}

// Forward runs one transformer layer.
// x: [batch, seqLen, dim] → [batch, seqLen, dim]
func (tb *TransformerBlock) Forward(x *tensor.Tensor) (*tensor.Tensor, error) {
	// Pre-norm attention with residual
	normed, err := tb.AttnNorm.Forward(x)
	if err != nil {
		return nil, err
	}

	attnOut, err := tb.Attn.Forward(normed, true) // causal=true
	if err != nil {
		return nil, err
	}

	// Residual connection: x = x + attn(norm(x))
	x, err = ops.Add(x, attnOut)
	if err != nil {
		return nil, err
	}

	// Pre-norm FFN with residual
	normed, err = tb.FFNNorm.Forward(x)
	if err != nil {
		return nil, err
	}

	ffnOut, err := tb.FFN.Forward(normed)
	if err != nil {
		return nil, err
	}

	// Residual connection: x = x + ffn(norm(x))
	x, err = ops.Add(x, ffnOut)
	if err != nil {
		return nil, err
	}

	return x, nil
}

// Parameters returns all trainable parameters.
func (tb *TransformerBlock) Parameters() []*tensor.Tensor {
	var params []*tensor.Tensor
	params = append(params, tb.AttnNorm.Parameters()...)
	params = append(params, tb.Attn.Parameters()...)
	params = append(params, tb.FFNNorm.Parameters()...)
	params = append(params, tb.FFN.Parameters()...)
	return params
}
```

## File: ./ops/loss.go
```go
package ops

import (
	"math"

	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/tensor"
)

// CrossEntropyLoss computes the cross-entropy loss between logits and targets.
// logits: [batch, seqLen, vocabSize] (float32)
// targets: [batch, seqLen] (int64)
// Returns: scalar loss tensor [1]
func CrossEntropyLoss(logits, targets *tensor.Tensor) (*tensor.Tensor, error) {
	logitsShape := logits.Shape()
	batch := logitsShape[0]
	seqLen := logitsShape[1]
	vocabSize := logitsShape[2]

	logitsData := logits.ToFloat32Slice()
	targetsData := targets.ToInt64Slice()

	totalLoss := float64(0)
	count := 0

	for b := 0; b < batch; b++ {
		for s := 0; s < seqLen; s++ {
			offset := (b*seqLen + s) * vocabSize
			target := int(targetsData[b*seqLen+s])

			if target < 0 { // padding token, skip
				continue
			}

			// Log-softmax: log(exp(x_target) / sum(exp(x_i)))
			// = x_target - log(sum(exp(x_i)))
			// With numerical stability: x_target - max - log(sum(exp(x_i - max)))
			maxVal := float64(-math.MaxFloat64)
			for v := 0; v < vocabSize; v++ {
				val := float64(logitsData[offset+v])
				if val > maxVal {
					maxVal = val
				}
			}

			sumExp := float64(0)
			for v := 0; v < vocabSize; v++ {
				sumExp += math.Exp(float64(logitsData[offset+v]) - maxVal)
			}
			logSumExp := maxVal + math.Log(sumExp)

			loss := logSumExp - float64(logitsData[offset+target])
			totalLoss += loss
			count++
		}
	}

	if count > 0 {
		totalLoss /= float64(count)
	}

	return tensor.FromSlice([]float32{float32(totalLoss)}, tensor.Shape{1})
}

// CrossEntropyBackward computes gradients of cross-entropy loss w.r.t. logits.
// Returns gradient tensor with same shape as logits: [batch, seqLen, vocabSize]
// Gradient = softmax(logits) - one_hot(targets), averaged over count.
func CrossEntropyBackward(logits, targets *tensor.Tensor) (*tensor.Tensor, error) {
	logitsShape := logits.Shape()
	batch := logitsShape[0]
	seqLen := logitsShape[1]
	vocabSize := logitsShape[2]

	logitsData := logits.ToFloat32Slice()
	targetsData := targets.ToInt64Slice()

	n := batch * seqLen * vocabSize
	gradData := make([]float32, n)

	count := 0
	for b := 0; b < batch; b++ {
		for s := 0; s < seqLen; s++ {
			offset := (b*seqLen + s) * vocabSize
			target := int(targetsData[b*seqLen+s])

			if target < 0 {
				continue
			}
			count++

			// Softmax
			maxVal := float32(-math.MaxFloat32)
			for v := 0; v < vocabSize; v++ {
				if logitsData[offset+v] > maxVal {
					maxVal = logitsData[offset+v]
				}
			}

			sumExp := float32(0)
			for v := 0; v < vocabSize; v++ {
				gradData[offset+v] = float32(math.Exp(float64(logitsData[offset+v] - maxVal)))
				sumExp += gradData[offset+v]
			}

			for v := 0; v < vocabSize; v++ {
				gradData[offset+v] /= sumExp // softmax probability
			}
			gradData[offset+target] -= 1.0 // subtract one-hot
		}
	}

	// Average over count
	if count > 0 {
		scale := float32(1.0) / float32(count)
		for i := range gradData {
			gradData[i] *= scale
		}
	}

	bk, err := backend.Get(backend.CPU)
	if err != nil {
		return nil, err
	}

	byteLen := n * int(tensor.Float32.Size())
	store, err := bk.Alloc(byteLen)
	if err != nil {
		return nil, err
	}

	// Copy grad data to storage
	dst := tensor.SliceFromPtr[float32](store.Ptr(), n)
	copy(dst, gradData)

	return tensor.NewTensor(store, logitsShape, tensor.Float32), nil
}
```

## File: ./ops/ops.go
```go
package ops

import (
	"fmt"

	"github.com/vugar/goml/backend"
	"github.com/vugar/goml/tensor"
)

// ---- Autograd function implementations ----

type addGradFn struct {
	a, b *tensor.Tensor
}

func (f *addGradFn) Name() string             { return "AddBackward" }
func (f *addGradFn) Inputs() []*tensor.Tensor  { return []*tensor.Tensor{f.a, f.b} }
func (f *addGradFn) Backward(grad *tensor.Tensor) []*tensor.Tensor {
	// d(a+b)/da = 1, d(a+b)/db = 1
	// TODO: handle broadcasting reduction
	return []*tensor.Tensor{grad, grad}
}

type mulGradFn struct {
	a, b *tensor.Tensor
}

func (f *mulGradFn) Name() string             { return "MulBackward" }
func (f *mulGradFn) Inputs() []*tensor.Tensor  { return []*tensor.Tensor{f.a, f.b} }
func (f *mulGradFn) Backward(grad *tensor.Tensor) []*tensor.Tensor {
	// d(a*b)/da = b, d(a*b)/db = a
	gradA, _ := Mul(grad, f.b)
	gradB, _ := Mul(grad, f.a)
	return []*tensor.Tensor{gradA, gradB}
}

type matmulGradFn struct {
	a, b *tensor.Tensor
}

func (f *matmulGradFn) Name() string             { return "MatMulBackward" }
func (f *matmulGradFn) Inputs() []*tensor.Tensor  { return []*tensor.Tensor{f.a, f.b} }
func (f *matmulGradFn) Backward(grad *tensor.Tensor) []*tensor.Tensor {
	// d(A@B)/dA = grad @ B^T
	// d(A@B)/dB = A^T @ grad
	bT, _ := f.b.T()
	aT, _ := f.a.T()
	gradA, _ := MatMul(grad, bT)
	gradB, _ := MatMul(aT, grad)
	return []*tensor.Tensor{gradA, gradB}
}

type reluGradFn struct {
	input *tensor.Tensor
}

func (f *reluGradFn) Name() string             { return "ReluBackward" }
func (f *reluGradFn) Inputs() []*tensor.Tensor  { return []*tensor.Tensor{f.input} }
func (f *reluGradFn) Backward(grad *tensor.Tensor) []*tensor.Tensor {
	// d(relu)/dx = 1 if x > 0 else 0
	// Approximated: relu'(x) = x > 0, so grad * (input > 0)
	// For now, we recompute relu mask
	// TODO: implement proper mask
	return []*tensor.Tensor{grad}
}

// ---- Public API ----

func getBackend(t *tensor.Tensor) (backend.Backend, error) {
	return backend.GetForDevice(t.Device())
}

func allocOutput(shape tensor.Shape, dtype tensor.DType, device backend.Device) (backend.Storage, error) {
	bk, err := backend.GetForDevice(device)
	if err != nil {
		return nil, err
	}
	return bk.Alloc(shape.NumElements() * int(dtype.Size()))
}

func needsGrad(tensors ...*tensor.Tensor) bool {
	for _, t := range tensors {
		if t.RequiresGrad() {
			return true
		}
	}
	return false
}

// Add performs element-wise addition.
func Add(a, b *tensor.Tensor) (*tensor.Tensor, error) {
	bk, err := getBackend(a)
	if err != nil {
		return nil, err
	}

	outShape, err := tensor.BroadcastShapes(a.Shape(), b.Shape())
	if err != nil {
		return nil, err
	}

	store, err := allocOutput(outShape, a.DType(), a.Device())
	if err != nil {
		return nil, err
	}

	if err := bk.Add(store, a.Storage(), b.Storage(), a.Shape(), b.Shape(), outShape, a.DType()); err != nil {
		return nil, err
	}

	out := tensor.NewTensor(store, outShape, a.DType())
	if needsGrad(a, b) {
		out.SetRequiresGrad(true)
		out.SetGradFn(&addGradFn{a: a, b: b})
	}
	return out, nil
}

// Mul performs element-wise multiplication.
func Mul(a, b *tensor.Tensor) (*tensor.Tensor, error) {
	bk, err := getBackend(a)
	if err != nil {
		return nil, err
	}

	outShape, err := tensor.BroadcastShapes(a.Shape(), b.Shape())
	if err != nil {
		return nil, err
	}

	store, err := allocOutput(outShape, a.DType(), a.Device())
	if err != nil {
		return nil, err
	}

	if err := bk.Mul(store, a.Storage(), b.Storage(), a.Shape(), b.Shape(), outShape, a.DType()); err != nil {
		return nil, err
	}

	out := tensor.NewTensor(store, outShape, a.DType())
	if needsGrad(a, b) {
		out.SetRequiresGrad(true)
		out.SetGradFn(&mulGradFn{a: a, b: b})
	}
	return out, nil
}

// MatMul performs matrix multiplication.
func MatMul(a, b *tensor.Tensor) (*tensor.Tensor, error) {
	bk, err := getBackend(a)
	if err != nil {
		return nil, err
	}

	// Ensure contiguous layout before sending to backend
	origA, origB := a, b
	if !a.IsContiguous() {
		a, err = a.Contiguous()
		if err != nil {
			return nil, fmt.Errorf("matmul: contiguous A: %w", err)
		}
	}
	if !b.IsContiguous() {
		b, err = b.Contiguous()
		if err != nil {
			return nil, fmt.Errorf("matmul: contiguous B: %w", err)
		}
	}

	shapeA := a.Shape()
	shapeB := b.Shape()
	ndimA := len(shapeA)
	ndimB := len(shapeB)

	M := shapeA[ndimA-2]
	N := shapeB[ndimB-1]

	// Output shape: batch dims + [M, N]
	outShape := make(tensor.Shape, ndimA)
	copy(outShape, shapeA[:ndimA-2])
	outShape[ndimA-2] = M
	outShape[ndimA-1] = N

	store, err := allocOutput(outShape, a.DType(), a.Device())
	if err != nil {
		return nil, err
	}

	if err := bk.MatMul(store, a.Storage(), b.Storage(), shapeA, shapeB, a.DType()); err != nil {
		return nil, err
	}

	out := tensor.NewTensor(store, outShape, a.DType())
	if needsGrad(origA, origB) {
		out.SetRequiresGrad(true)
		out.SetGradFn(&matmulGradFn{a: origA, b: origB})
	}
	return out, nil
}

// Relu applies rectified linear unit.
func Relu(t *tensor.Tensor) (*tensor.Tensor, error) {
	bk, err := getBackend(t)
	if err != nil {
		return nil, err
	}

	store, err := allocOutput(t.Shape(), t.DType(), t.Device())
	if err != nil {
		return nil, err
	}

	if err := bk.Relu(store, t.Storage(), t.Shape(), t.DType()); err != nil {
		return nil, err
	}

	out := tensor.NewTensor(store, t.Shape(), t.DType())
	if needsGrad(t) {
		out.SetRequiresGrad(true)
		out.SetGradFn(&reluGradFn{input: t})
	}
	return out, nil
}

// Softmax applies softmax along the given axis.
func Softmax(t *tensor.Tensor, axis int) (*tensor.Tensor, error) {
	bk, err := getBackend(t)
	if err != nil {
		return nil, err
	}

	store, err := allocOutput(t.Shape(), t.DType(), t.Device())
	if err != nil {
		return nil, err
	}

	if err := bk.Softmax(store, t.Storage(), t.Shape(), axis, t.DType()); err != nil {
		return nil, err
	}

	return tensor.NewTensor(store, t.Shape(), t.DType()), nil
}

// LayerNorm applies layer normalization.
func LayerNorm(x, gamma, beta *tensor.Tensor, normAxis int, eps float64) (*tensor.Tensor, error) {
	bk, err := getBackend(x)
	if err != nil {
		return nil, err
	}

	store, err := allocOutput(x.Shape(), x.DType(), x.Device())
	if err != nil {
		return nil, err
	}

	var gs, bs backend.Storage
	if gamma != nil {
		gs = gamma.Storage()
	}
	if beta != nil {
		bs = beta.Storage()
	}

	if err := bk.LayerNorm(store, x.Storage(), gs, bs, x.Shape(), normAxis, eps, x.DType()); err != nil {
		return nil, err
	}

	return tensor.NewTensor(store, x.Shape(), x.DType()), nil
}

// Gelu applies GELU activation.
func Gelu(t *tensor.Tensor) (*tensor.Tensor, error) {
	bk, err := getBackend(t)
	if err != nil {
		return nil, err
	}

	store, err := allocOutput(t.Shape(), t.DType(), t.Device())
	if err != nil {
		return nil, err
	}

	if err := bk.Gelu(store, t.Storage(), t.Shape(), t.DType()); err != nil {
		return nil, err
	}

	return tensor.NewTensor(store, t.Shape(), t.DType()), nil
}

// Silu applies SiLU (Swish) activation.
func Silu(t *tensor.Tensor) (*tensor.Tensor, error) {
	bk, err := getBackend(t)
	if err != nil {
		return nil, err
	}

	store, err := allocOutput(t.Shape(), t.DType(), t.Device())
	if err != nil {
		return nil, err
	}

	if err := bk.Silu(store, t.Storage(), t.Shape(), t.DType()); err != nil {
		return nil, err
	}

	return tensor.NewTensor(store, t.Shape(), t.DType()), nil
}

// ScaledDotProductAttention computes multi-head attention.
func ScaledDotProductAttention(q, k, v *tensor.Tensor, numHeads int, causal bool) (*tensor.Tensor, error) {
	bk, err := getBackend(q)
	if err != nil {
		return nil, err
	}

	shape := q.Shape()
	batchSize := shape[0]
	seqLen := shape[2]
	headDim := shape[3]

	store, err := allocOutput(shape, q.DType(), q.Device())
	if err != nil {
		return nil, err
	}

	if err := bk.ScaledDotProductAttention(
		store, q.Storage(), k.Storage(), v.Storage(),
		batchSize, numHeads, seqLen, headDim,
		causal, q.DType(),
	); err != nil {
		return nil, err
	}

	return tensor.NewTensor(store, shape, q.DType()), nil
}
```

## File: ./optim/adamw.go
```go
package optim

import (
	"math"

	"github.com/vugar/goml/tensor"
)

// AdamW implements the AdamW optimizer (decoupled weight decay).
// Standard hyperparameters follow the original paper + LLM best practices.
type AdamW struct {
	Params      []*tensor.Tensor
	LR          float64 // learning rate
	Beta1       float64 // first moment decay (default 0.9)
	Beta2       float64 // second moment decay (default 0.95 for LLMs)
	Eps         float64 // numerical stability (default 1e-8)
	WeightDecay float64 // L2 regularization (default 0.1 for LLMs)
	MaxGradNorm float64 // gradient clipping (0 = disabled)

	// State
	m    [][]float32 // first moment (mean of gradients)
	v    [][]float32 // second moment (mean of squared gradients)
	step int
}

// NewAdamW creates an optimizer with LLM-tuned defaults.
func NewAdamW(params []*tensor.Tensor, lr float64) *AdamW {
	m := make([][]float32, len(params))
	v := make([][]float32, len(params))
	for i, p := range params {
		n := p.NumElements()
		m[i] = make([]float32, n)
		v[i] = make([]float32, n)
	}

	return &AdamW{
		Params:      params,
		LR:          lr,
		Beta1:       0.9,
		Beta2:       0.95,
		Eps:         1e-8,
		WeightDecay: 0.1,
		MaxGradNorm: 1.0,
		m:           m,
		v:           v,
	}
}

// Step performs one optimization step.
// Gradients must be set on each parameter tensor before calling.
func (opt *AdamW) Step() {
	opt.step++

	// Gradient clipping (global norm)
	if opt.MaxGradNorm > 0 {
		opt.clipGradNorm()
	}

	// Bias correction factors
	bc1 := 1.0 - math.Pow(opt.Beta1, float64(opt.step))
	bc2 := 1.0 - math.Pow(opt.Beta2, float64(opt.step))

	lr := opt.LR

	for i, param := range opt.Params {
		if param.Grad() == nil {
			continue
		}

		pData := param.ToFloat32Slice()
		gData := param.Grad().ToFloat32Slice()
		m := opt.m[i]
		v := opt.v[i]

		for j := 0; j < len(pData); j++ {
			g := gData[j]

			// Update moments
			m[j] = float32(opt.Beta1)*m[j] + float32(1-opt.Beta1)*g
			v[j] = float32(opt.Beta2)*v[j] + float32(1-opt.Beta2)*g*g

			// Bias-corrected moments
			mHat := float64(m[j]) / bc1
			vHat := float64(v[j]) / bc2

			// Adam update
			update := mHat / (math.Sqrt(vHat) + opt.Eps)

			// Decoupled weight decay (AdamW)
			pData[j] -= float32(lr) * (float32(update) + float32(opt.WeightDecay)*pData[j])
		}
	}
}

// ZeroGrad clears all gradients.
func (opt *AdamW) ZeroGrad() {
	for _, p := range opt.Params {
		if p.Grad() != nil {
			gData := p.Grad().ToFloat32Slice()
			for i := range gData {
				gData[i] = 0
			}
		}
	}
}

// clipGradNorm clips gradients by global L2 norm.
func (opt *AdamW) clipGradNorm() {
	// Compute global norm
	totalNorm := float64(0)
	for _, p := range opt.Params {
		if p.Grad() == nil {
			continue
		}
		gData := p.Grad().ToFloat32Slice()
		for _, g := range gData {
			totalNorm += float64(g) * float64(g)
		}
	}
	totalNorm = math.Sqrt(totalNorm)

	if totalNorm <= opt.MaxGradNorm {
		return
	}

	// Scale gradients
	scale := float32(opt.MaxGradNorm / totalNorm)
	for _, p := range opt.Params {
		if p.Grad() == nil {
			continue
		}
		gData := p.Grad().ToFloat32Slice()
		for i := range gData {
			gData[i] *= scale
		}
	}
}

// GetLR returns current learning rate.
func (opt *AdamW) GetLR() float64 {
	return opt.LR
}

// SetLR updates the learning rate (for scheduling).
func (opt *AdamW) SetLR(lr float64) {
	opt.LR = lr
}

// CosineSchedule computes learning rate with warmup + cosine decay.
func CosineSchedule(step, warmupSteps, totalSteps int, maxLR, minLR float64) float64 {
	if step < warmupSteps {
		// Linear warmup
		return maxLR * float64(step) / float64(warmupSteps)
	}

	// Cosine decay
	progress := float64(step-warmupSteps) / float64(totalSteps-warmupSteps)
	if progress > 1.0 {
		progress = 1.0
	}
	return minLR + 0.5*(maxLR-minLR)*(1.0+math.Cos(math.Pi*progress))
}
```

## File: ./tensor/mem.go
```go
package tensor

import "unsafe"

// copySliceToStorage copies a Go slice into a storage buffer safely.
func copySliceToStorage[T any](data []T, dst []byte) {
	if len(data) == 0 || len(dst) == 0 {
		return
	}
	var zero T
	elemSize := int(unsafe.Sizeof(zero))
	srcLen := len(data) * elemSize
	if srcLen > len(dst) {
		srcLen = len(dst)
	}
	srcBytes := unsafe.Slice((*byte)(unsafe.Pointer(&data[0])), srcLen)
	copy(dst, srcBytes)
}

// ptrSlice interprets a storage's memory as a typed slice.
// Uses Bytes() for safe access on CPU.
func ptrSlice[T any](b []byte, n int) []T {
	if n == 0 || len(b) == 0 {
		return nil
	}
	return unsafe.Slice((*T)(unsafe.Pointer(&b[0])), n)
}

// SliceFromPtr interprets raw memory as a Go slice (for GPU backends).
func SliceFromPtr[T any](ptr uintptr, n int) []T {
	if n == 0 {
		return nil
	}
	return unsafe.Slice((*T)(unsafe.Pointer(ptr)), n)
}

// ToFloat32Slice returns the tensor data as []float32.
func (t *Tensor) ToFloat32Slice() []float32 {
	if b := t.storage.Bytes(); b != nil {
		return ptrSlice[float32](b, t.NumElements())
	}
	return SliceFromPtr[float32](t.storage.Ptr(), t.NumElements())
}

// ToFloat64Slice returns the tensor data as []float64.
func (t *Tensor) ToFloat64Slice() []float64 {
	if b := t.storage.Bytes(); b != nil {
		return ptrSlice[float64](b, t.NumElements())
	}
	return SliceFromPtr[float64](t.storage.Ptr(), t.NumElements())
}

// ToInt32Slice returns the tensor data as []int32.
func (t *Tensor) ToInt32Slice() []int32 {
	if b := t.storage.Bytes(); b != nil {
		return ptrSlice[int32](b, t.NumElements())
	}
	return SliceFromPtr[int32](t.storage.Ptr(), t.NumElements())
}

// ToInt64Slice returns the tensor data as []int64.
func (t *Tensor) ToInt64Slice() []int64 {
	if b := t.storage.Bytes(); b != nil {
		return ptrSlice[int64](b, t.NumElements())
	}
	return SliceFromPtr[int64](t.storage.Ptr(), t.NumElements())
}
```

## File: ./tensor/reexport.go
```go
package tensor

import "github.com/vugar/goml/core"

// Re-export core types so tensor.Shape, tensor.DType etc. still work.
type Shape = core.Shape
type Strides = core.Strides
type DType = core.DType
type BFloat16Value = core.BFloat16Value

const (
	Float16  = core.Float16
	Float32  = core.Float32
	Float64  = core.Float64
	BFloat16 = core.BFloat16
	Int8     = core.Int8
	Int16    = core.Int16
	Int32    = core.Int32
	Int64    = core.Int64
	Uint8    = core.Uint8
	Bool     = core.Bool
)

var (
	ContiguousStrides = core.ContiguousStrides
	IsContiguous      = core.IsContiguous
	BroadcastShapes   = core.BroadcastShapes
	FlatIndex         = core.FlatIndex
	Permute           = core.Permute
	BFloat16FromFloat32 = core.BFloat16FromFloat32
)
```

## File: ./tensor/tensor.go
```go
package tensor

import (
	"fmt"

	"github.com/vugar/goml/backend"
)

// Tensor is the core n-dimensional array.
// It can live on any device and supports autograd.
type Tensor struct {
	storage backend.Storage
	shape   Shape
	strides Strides
	dtype   DType
	offset  int // byte offset into storage (for views)

	// Autograd fields
	requiresGrad bool
	grad         *Tensor
	gradFn       GradFn // function that produced this tensor
	isLeaf       bool   // true if created by user (not by an op)
}

// GradFn represents the backward function for autograd.
type GradFn interface {
	Backward(gradOutput *Tensor) []*Tensor // returns gradients for each input
	Inputs() []*Tensor
	Name() string
}

// ---- Constructors ----

// NewTensor creates a tensor with given storage and metadata.
func NewTensor(storage backend.Storage, shape Shape, dtype DType) *Tensor {
	strides := ContiguousStrides(shape, dtype.Size())
	return &Tensor{
		storage: storage,
		shape:   shape.Clone(),
		strides: strides,
		dtype:   dtype,
		isLeaf:  true,
	}
}

// FromSlice creates a CPU tensor from a Go slice.
func FromSlice[T float32 | float64 | int32 | int64](data []T, shape Shape) (*Tensor, error) {
	n := shape.NumElements()
	if len(data) != n {
		return nil, fmt.Errorf("data length %d != shape elements %d", len(data), n)
	}

	var dtype DType
	switch any(data[0]).(type) {
	case float32:
		dtype = Float32
	case float64:
		dtype = Float64
	case int32:
		dtype = Int32
	case int64:
		dtype = Int64
	}

	b, err := backend.Get(backend.CPU)
	if err != nil {
		return nil, err
	}

	byteLen := n * int(dtype.Size())
	store, err := b.Alloc(byteLen)
	if err != nil {
		return nil, err
	}

	// Copy data into storage
	copySliceToStorage(data, store.Bytes())

	return NewTensor(store, shape, dtype), nil
}

// Zeros creates a zero-filled tensor.
func Zeros(shape Shape, dtype DType, device backend.Device) (*Tensor, error) {
	b, err := backend.GetForDevice(device)
	if err != nil {
		return nil, err
	}

	n := shape.NumElements()
	byteLen := n * int(dtype.Size())
	store, err := b.Alloc(byteLen)
	if err != nil {
		return nil, err
	}

	if err := b.Fill(store, shape, 0, dtype); err != nil {
		store.Free()
		return nil, err
	}

	return NewTensor(store, shape, dtype), nil
}

// Ones creates a tensor filled with ones.
func Ones(shape Shape, dtype DType, device backend.Device) (*Tensor, error) {
	b, err := backend.GetForDevice(device)
	if err != nil {
		return nil, err
	}

	n := shape.NumElements()
	byteLen := n * int(dtype.Size())
	store, err := b.Alloc(byteLen)
	if err != nil {
		return nil, err
	}

	if err := b.Fill(store, shape, 1, dtype); err != nil {
		store.Free()
		return nil, err
	}

	return NewTensor(store, shape, dtype), nil
}

// Arange creates a 1D tensor with values [start, start+step, start+2*step, ...].
func Arange(start, step float64, n int, dtype DType, device backend.Device) (*Tensor, error) {
	b, err := backend.GetForDevice(device)
	if err != nil {
		return nil, err
	}

	byteLen := n * int(dtype.Size())
	store, err := b.Alloc(byteLen)
	if err != nil {
		return nil, err
	}

	if err := b.Arange(store, start, step, n, dtype); err != nil {
		store.Free()
		return nil, err
	}

	return NewTensor(store, Shape{n}, dtype), nil
}

// ---- Accessors ----

func (t *Tensor) Shape() Shape          { return t.shape }
func (t *Tensor) Strides() Strides      { return t.strides }
func (t *Tensor) DType() DType          { return t.dtype }
func (t *Tensor) NDim() int             { return len(t.shape) }
func (t *Tensor) NumElements() int      { return t.shape.NumElements() }
func (t *Tensor) Device() backend.Device { return t.storage.Device() }
func (t *Tensor) Storage() backend.Storage { return t.storage }
func (t *Tensor) IsLeaf() bool          { return t.isLeaf }

func (t *Tensor) IsContiguous() bool {
	return IsContiguous(t.shape, t.strides, t.dtype.Size())
}

// Contiguous returns a contiguous copy of the tensor.
// If already contiguous, returns the same tensor (no copy).
func (t *Tensor) Contiguous() (*Tensor, error) {
	if t.IsContiguous() {
		return t, nil
	}

	n := t.NumElements()
	elemSize := int(t.dtype.Size())
	byteLen := n * elemSize

	// Allocate new contiguous storage
	newData := make([]float32, n)
	
	shape := t.shape
	strides := t.strides
	ndim := len(shape)
	indices := make([]int, ndim)
	srcSlice := SliceFromPtr[float32](t.storage.Ptr(), n*2) // oversized for safety

	for i := 0; i < n; i++ {
		// Compute source offset from strides (in bytes → elements)
		srcOffset := 0
		for d := 0; d < ndim; d++ {
			srcOffset += indices[d] * (strides[d] / elemSize)
		}
		newData[i] = srcSlice[srcOffset]

		// Increment indices
		for d := ndim - 1; d >= 0; d-- {
			indices[d]++
			if indices[d] < shape[d] {
				break
			}
			indices[d] = 0
		}
	}

	result, err := FromSlice(newData, shape)
	if err != nil {
		// fallback: alloc manually
		_ = byteLen
		return nil, err
	}
	return result, nil
}

func (t *Tensor) RequiresGrad() bool { return t.requiresGrad }

func (t *Tensor) SetRequiresGrad(v bool) *Tensor {
	t.requiresGrad = v
	return t
}

func (t *Tensor) Grad() *Tensor { return t.grad }

func (t *Tensor) SetGradFn(fn GradFn) {
	t.gradFn = fn
	t.isLeaf = false
}

func (t *Tensor) GradFn() GradFn { return t.gradFn }

func (t *Tensor) SetGrad(grad *Tensor) { t.grad = grad }

// ---- Views ----

// View returns a tensor with a new shape but shared storage.
func (t *Tensor) View(newShape Shape) (*Tensor, error) {
	if !t.IsContiguous() {
		return nil, fmt.Errorf("view requires contiguous tensor")
	}
	if newShape.NumElements() != t.NumElements() {
		return nil, fmt.Errorf("view shape %v has %d elements, need %d",
			newShape, newShape.NumElements(), t.NumElements())
	}
	return &Tensor{
		storage:      t.storage,
		shape:        newShape.Clone(),
		strides:      ContiguousStrides(newShape, t.dtype.Size()),
		dtype:        t.dtype,
		offset:       t.offset,
		requiresGrad: t.requiresGrad,
		isLeaf:       false,
	}, nil
}

// Transpose returns a view with permuted axes.
func (t *Tensor) Transpose(axes []int) (*Tensor, error) {
	newShape, newStrides, err := Permute(t.shape, t.strides, axes)
	if err != nil {
		return nil, err
	}
	return &Tensor{
		storage:      t.storage,
		shape:        newShape,
		strides:      newStrides,
		dtype:        t.dtype,
		offset:       t.offset,
		requiresGrad: t.requiresGrad,
		isLeaf:       false,
	}, nil
}

// T transposes a 2D tensor (shorthand for Transpose([]int{1, 0})).
func (t *Tensor) T() (*Tensor, error) {
	if t.NDim() != 2 {
		return nil, fmt.Errorf("T() requires 2D tensor, got %dD", t.NDim())
	}
	return t.Transpose([]int{1, 0})
}

// Free releases the underlying storage.
func (t *Tensor) Free() {
	if t.storage != nil {
		t.storage.Free()
		t.storage = nil
	}
	if t.grad != nil {
		t.grad.Free()
		t.grad = nil
	}
}

func (t *Tensor) String() string {
	return fmt.Sprintf("Tensor(shape=%v, dtype=%s, device=%s, grad=%v)",
		t.shape, t.dtype, t.Device(), t.requiresGrad)
}
```

## File: ./tokenizer/byte.go
```go
package tokenizer

// ByteTokenizer is the simplest possible tokenizer — each byte is a token.
// Vocab size = 256. No subword merging.
// Good enough for training demos, will upgrade to BPE later.
type ByteTokenizer struct {
	VocabSize int
}

func NewByteTokenizer() *ByteTokenizer {
	return &ByteTokenizer{VocabSize: 256}
}

// Encode converts a string to token IDs.
func (t *ByteTokenizer) Encode(text string) []int64 {
	bytes := []byte(text)
	tokens := make([]int64, len(bytes))
	for i, b := range bytes {
		tokens[i] = int64(b)
	}
	return tokens
}

// Decode converts token IDs back to a string.
func (t *ByteTokenizer) Decode(tokens []int64) string {
	bytes := make([]byte, len(tokens))
	for i, t := range tokens {
		bytes[i] = byte(t)
	}
	return string(bytes)
}

// DecodeToken converts a single token ID to string.
func (t *ByteTokenizer) DecodeToken(token int64) string {
	return string([]byte{byte(token)})
}
```

## File: ./train/trainer.go
```go
package train

import (
	"fmt"
	"math"
	"math/rand"
	"time"

	"github.com/vugar/goml/nn"
	"github.com/vugar/goml/ops"
	"github.com/vugar/goml/optim"
	"github.com/vugar/goml/tensor"
	"github.com/vugar/goml/tokenizer"
)

// TrainConfig holds training hyperparameters.
type TrainConfig struct {
	BatchSize   int
	SeqLen      int
	LR          float64
	WarmupSteps int
	TotalSteps  int
	MinLR       float64
	LogEvery    int
	EvalEvery   int
	GenEvery    int
	GenLen      int
	Temperature float32
}

func DefaultTrainConfig() TrainConfig {
	return TrainConfig{
		BatchSize:   4,
		SeqLen:      64,
		LR:          3e-4,
		WarmupSteps: 100,
		TotalSteps:  2000,
		MinLR:       3e-5,
		LogEvery:    10,
		EvalEvery:   100,
		GenEvery:    200,
		GenLen:      128,
		Temperature: 0.8,
	}
}

// Trainer handles the training loop.
type Trainer struct {
	Model     *nn.LLM
	Optimizer *optim.AdamW
	Tokenizer *tokenizer.ByteTokenizer
	Config    TrainConfig

	trainTokens []int64
	evalTokens  []int64
}

func NewTrainer(model *nn.LLM, text string, cfg TrainConfig) *Trainer {
	tok := tokenizer.NewByteTokenizer()
	allTokens := tok.Encode(text)

	splitIdx := int(float64(len(allTokens)) * 0.9)
	trainTokens := allTokens[:splitIdx]
	evalTokens := allTokens[splitIdx:]

	optimizer := optim.NewAdamW(model.Parameters(), cfg.LR)

	return &Trainer{
		Model:       model,
		Optimizer:   optimizer,
		Tokenizer:   tok,
		Config:      cfg,
		trainTokens: trainTokens,
		evalTokens:  evalTokens,
	}
}

func (t *Trainer) Train() {
	cfg := t.Config

	fmt.Printf("Training data: %d tokens (train: %d, eval: %d)\n",
		len(t.trainTokens)+len(t.evalTokens), len(t.trainTokens), len(t.evalTokens))
	fmt.Printf("Model: %d parameters\n", t.Model.CountParameters())
	fmt.Printf("Config: batch=%d, seqLen=%d, lr=%.1e, steps=%d\n\n",
		cfg.BatchSize, cfg.SeqLen, cfg.LR, cfg.TotalSteps)

	totalStart := time.Now()
	smoothLoss := float64(0)
	bestEvalLoss := math.MaxFloat64

	for step := 1; step <= cfg.TotalSteps; step++ {
		stepStart := time.Now()

		lr := optim.CosineSchedule(step, cfg.WarmupSteps, cfg.TotalSteps, cfg.LR, cfg.MinLR)
		t.Optimizer.SetLR(lr)

		inputs, targets := t.getBatch(t.trainTokens)

		logits, cache, err := t.Model.ForwardWithCache(inputs)
		if err != nil {
			fmt.Printf("Step %d forward error: %v\n", step, err)
			continue
		}

		loss, err := ops.CrossEntropyLoss(logits, targets)
		if err != nil {
			fmt.Printf("Step %d loss error: %v\n", step, err)
			continue
		}
		lossVal := float64(loss.ToFloat32Slice()[0])

		if smoothLoss == 0 {
			smoothLoss = lossVal
		} else {
			smoothLoss = 0.95*smoothLoss + 0.05*lossVal
		}

		t.Optimizer.ZeroGrad()
		dLogits, err := ops.CrossEntropyBackward(logits, targets)
		if err != nil {
			fmt.Printf("Step %d backward error: %v\n", step, err)
			continue
		}

		err = t.Model.Backward(cache, dLogits)
		if err != nil {
			fmt.Printf("Step %d model backward error: %v\n", step, err)
			continue
		}

		t.Optimizer.Step()

		stepTime := time.Since(stepStart)

		if step%cfg.LogEvery == 0 {
			tokPerSec := float64(cfg.BatchSize*cfg.SeqLen) / stepTime.Seconds()
			fmt.Printf("step %4d | loss %.4f (smooth %.4f) | lr %.2e | %.0f tok/s | %v\n",
				step, lossVal, smoothLoss, lr, tokPerSec, stepTime)
		}

		if step%cfg.EvalEvery == 0 {
			evalLoss := t.evaluate()
			improved := ""
			if evalLoss < bestEvalLoss {
				bestEvalLoss = evalLoss
				improved = " * best"
			}
			fmt.Printf("         -> eval loss: %.4f%s\n", evalLoss, improved)
		}

		if step%cfg.GenEvery == 0 {
			sample := t.generate("The ", cfg.GenLen, cfg.Temperature)
			fmt.Printf("         -> sample: %q\n", truncate(sample, 120))
		}
	}

	totalTime := time.Since(totalStart)
	fmt.Printf("\nTraining complete in %v\n", totalTime)
	fmt.Printf("Best eval loss: %.4f\n", bestEvalLoss)

	fmt.Println("\n--- Final generation samples ---")
	prompts := []string{"To be or ", "The king ", "What is ", "In the "}
	for _, p := range prompts {
		sample := t.generate(p, 200, 0.7)
		fmt.Printf("\nPrompt: %q\n-> %s\n", p, truncate(sample, 200))
	}
}

func (t *Trainer) getBatch(tokens []int64) (*tensor.Tensor, *tensor.Tensor) {
	cfg := t.Config
	maxStart := len(tokens) - cfg.SeqLen - 1

	inputData := make([]int64, cfg.BatchSize*cfg.SeqLen)
	targetData := make([]int64, cfg.BatchSize*cfg.SeqLen)

	for b := 0; b < cfg.BatchSize; b++ {
		start := rand.Intn(maxStart)
		for s := 0; s < cfg.SeqLen; s++ {
			inputData[b*cfg.SeqLen+s] = tokens[start+s]
			targetData[b*cfg.SeqLen+s] = tokens[start+s+1]
		}
	}

	inputs, _ := tensor.FromSlice(inputData, tensor.Shape{cfg.BatchSize, cfg.SeqLen})
	targets, _ := tensor.FromSlice(targetData, tensor.Shape{cfg.BatchSize, cfg.SeqLen})
	return inputs, targets
}

func (t *Trainer) evaluate() float64 {
	numBatches := 5
	totalLoss := float64(0)

	for i := 0; i < numBatches; i++ {
		inputs, targets := t.getBatch(t.evalTokens)
		logits, _, err := t.Model.ForwardWithCache(inputs)
		if err != nil {
			continue
		}
		loss, err := ops.CrossEntropyLoss(logits, targets)
		if err != nil {
			continue
		}
		totalLoss += float64(loss.ToFloat32Slice()[0])
	}

	return totalLoss / float64(numBatches)
}

func (t *Trainer) generate(prompt string, maxLen int, temperature float32) string {
	tokens := t.Tokenizer.Encode(prompt)
	cfg := t.Model.Config

	for i := 0; i < maxLen; i++ {
		start := 0
		if len(tokens) > cfg.MaxSeqLen {
			start = len(tokens) - cfg.MaxSeqLen
		}
		window := tokens[start:]

		input, _ := tensor.FromSlice(window, tensor.Shape{1, len(window)})
		logits, _ := t.Model.Forward(input)

		logitsData := logits.ToFloat32Slice()
		lastStart := (len(window) - 1) * cfg.VocabSize
		lastLogits, _ := tensor.FromSlice(
			logitsData[lastStart:lastStart+cfg.VocabSize],
			tensor.Shape{cfg.VocabSize},
		)

		nextToken := int64(nn.TopKSample(lastLogits, 40, temperature))
		tokens = append(tokens, nextToken)
	}

	return t.Tokenizer.Decode(tokens)
}

func truncate(s string, maxLen int) string {
	if len(s) > maxLen {
		return s[:maxLen] + "..."
	}
	return s
}
```

# Shell Scripts

## File: go.mod
```
module github.com/vugar/goml

go 1.22.2
```

